# This repo contains a snapshot of a version of ghc-events,
# from http://code.haskell.org/~duncan/ghc-events/
# and a copy of http://code.haskell.org/~duncan/parlist.hs
# Both are slightly modified. It also contains plot scripts
# generated using gnuplot (http://www.gnuplot.info/).
# The ghc version required to compile the included ghc-events
# is the one from http://darcs.haskell.org/~duncan/ghc/

# Instructions:
# to see the plots, do (using the right ghc version)

cd ghc-events/; cabal install; cd ..

ghc -O -eventlog -rtsopts parlist.hs --make -threaded

./parlist +RTS -N2 -ls; show-ghc-events parlist.eventlog > output.dat

gnuplot

load "pool_size.plt"
load "pool_size_max.plt"
load "pool_size_min.plt"
load "pool_size_mean.plt"

load "pool_size_out.plt"
load "pool_size_max_out.plt"
load "pool_size_min_out.plt"
load "pool_size_mean_out.plt"

# TODO: make those zoom in (saw pattern) and zoom out (add increments) well
# TODO: colour (<@dcoutts> eg black duds, green created and red overflows)
# TODO: indicate somewhere than the area below the graph corresponds to
# the total cumulated number of sparks created/disposed
load "creation_velocity.plt"
load "disposal_velocity.plt"

# TODO: make >= 4 more plots:
# two zoomed out local speeds, two or more zoomed in local speeds with
# different time intervals for resampling


# to generate the plots, do, e.g.,

set term pngcairo size 600, 400

# and repeat the loads, naming the pictures before each, as in

set output "png/pool_size.png"

# at one point I also needed "set yrange [0:1100]" for pool_size.plt

# deprecated plots:

# too much vertical space taken; too similar for different data (close to y=x)
load "creation_cumulative.plt"
load "disposal_cumulative.plt"

# Data is not continuous, so derivative does not exist,
# so the difference quotient does not make sense and looks bad:
# we have a saw pattern and the more samples, the more it is just
# a sequence of separate peaks of equal height (binary signal: 0 and 1).
# The area under the graph still corresponds to the number of spark
# transitions, as required, but it's effectively via counting the peaks.
# With uneven distribution of samples it's even worse, because the same
# data in different parts of the graph looks very differently
# (binary, horizontal signal vs. aggregated, vertical signal on the same graph).
load "cr_local_speed.plt"
load "di_local_speed.plt"

# speed averaged over arbitrary, too big interval (from start to sample point);
# the higher the time, the more smooth the curve
load "creation_speed.plt"
load "disposal_speed.plt"
