commit 55ec13f62277065f8c4657fa90f6e0692328aa71
Author: Mikolaj <mikolaj.konarski@gmail.com>
Commit: Mikolaj <mikolaj.konarski@gmail.com>

    add a missing traceSparkCounters invocation (and lots of whitespace cleanup)

diff --git a/rts/Capability.c b/rts/Capability.c
index 3c06f5a..2d11ecc 100644
--- a/rts/Capability.c
+++ b/rts/Capability.c
@@ -112,7 +112,7 @@ findSpark (Capability *cap)
       if (n_capabilities == 1) { return NULL; } // makes no sense...
 
       debugTrace(DEBUG_sched,
-                 "cap %d: Trying to steal work from other capabilities", 
+                 "cap %d: Trying to steal work from other capabilities",
                  cap->no);
 
       /* visit cap.s 0..n-1 in sequence until a theft succeeds. We could
@@ -140,7 +140,7 @@ findSpark (Capability *cap)
               cap->spark_stats.converted++;
 
               traceEventStealSpark(cap, cap->r.rCurrentTSO, robbed->no);
-              
+
               return spark;
           }
           // otherwise: no success, try next one
@@ -281,8 +281,8 @@ initCapability( Capability *cap, nat i )
 void
 initCapabilities( void )
 {
-    /* Declare a single capability set representing the process. 
-       Each capability will get added to this capset. */ 
+    /* Declare a single capability set representing the process.
+       Each capability will get added to this capset. */
     traceCapsetCreate(CAPSET_OSPROCESS_DEFAULT, CapsetTypeOsProcess);
 
 #if defined(THREADED_RTS)
@@ -384,7 +384,7 @@ giveCapabilityToTask (Capability *cap USED_IF_DEBUG, Task *task)
 
 #if defined(THREADED_RTS)
 void
-releaseCapability_ (Capability* cap, 
+releaseCapability_ (Capability* cap,
                     rtsBool always_wakeup)
 {
     Task *task;
@@ -407,7 +407,7 @@ releaseCapability_ (Capability* cap,
       last_free_capability = cap; // needed?
       debugTrace(DEBUG_sched, "GC pending, set capability %d free", cap->no);
       return;
-    } 
+    }
 
 
     // If the next thread on the run queue is a bound thread,
@@ -438,7 +438,7 @@ releaseCapability_ (Capability* cap,
 
     // If we have an unbound thread on the run queue, or if there's
     // anything else to do, give the Capability to a worker thread.
-    if (always_wakeup || 
+    if (always_wakeup ||
         !emptyRunQueue(cap) || !emptyInbox(cap) ||
         !emptySparkPoolCap(cap) || globalWorkToDo()) {
 	if (cap->spare_workers) {
@@ -621,6 +621,7 @@ yieldCapability (Capability** pCap, Task *task)
         traceEventGcStart(cap);
         gcWorkerThread(cap);
         traceEventGcEnd(cap);
+        traceSparkCounters(cap);
         return;
     }
 
@@ -643,7 +644,7 @@ yieldCapability (Capability** pCap, Task *task)
 
 	    ACQUIRE_LOCK(&cap->lock);
 	    if (cap->running_task != NULL) {
-		debugTrace(DEBUG_sched, 
+		debugTrace(DEBUG_sched,
 			   "capability %d is owned by another task", cap->no);
 		RELEASE_LOCK(&cap->lock);
 		continue;
@@ -680,7 +681,7 @@ yieldCapability (Capability** pCap, Task *task)
 /* ----------------------------------------------------------------------------
  * prodCapability
  *
- * If a Capability is currently idle, wake up a Task on it.  Used to 
+ * If a Capability is currently idle, wake up a Task on it.  Used to
  * get every Capability into the GC.
  * ------------------------------------------------------------------------- */
 
@@ -754,7 +755,7 @@ shutdownCapability (Capability *cap,
     for (i = 0; /* i < 50 */; i++) {
         ASSERT(sched_state == SCHED_SHUTTING_DOWN);
 
-	debugTrace(DEBUG_sched, 
+	debugTrace(DEBUG_sched,
 		   "shutting down capability %d, attempt %d", cap->no, i);
 	ACQUIRE_LOCK(&cap->lock);
 	if (cap->running_task) {
@@ -776,7 +777,7 @@ shutdownCapability (Capability *cap,
             prev = NULL;
             for (t = cap->spare_workers; t != NULL; t = t->next) {
                 if (!osThreadIsAlive(t->id)) {
-                    debugTrace(DEBUG_sched, 
+                    debugTrace(DEBUG_sched,
                                "worker thread %p has died unexpectedly", (void *)t->id);
                     cap->n_spare_workers--;
                     if (!prev) {
@@ -790,7 +791,7 @@ shutdownCapability (Capability *cap,
         }
 
 	if (!emptyRunQueue(cap) || cap->spare_workers) {
-	    debugTrace(DEBUG_sched, 
+	    debugTrace(DEBUG_sched,
 		       "runnable threads or workers still alive, yielding");
 	    releaseCapability_(cap,rtsFalse); // this will wake up a worker
 	    RELEASE_LOCK(&cap->lock);
@@ -805,7 +806,7 @@ shutdownCapability (Capability *cap,
         // We can be a bit more relaxed when this is a standalone
         // program that is about to terminate, and let safe=false.
         if (cap->suspended_ccalls && safe) {
-	    debugTrace(DEBUG_sched, 
+	    debugTrace(DEBUG_sched,
 		       "thread(s) are involved in foreign calls, yielding");
             cap->running_task = NULL;
 	    RELEASE_LOCK(&cap->lock);
@@ -829,7 +830,7 @@ shutdownCapability (Capability *cap,
     // list are both empty.
 
     // ToDo: we can't drop this mutex, because there might still be
-    // threads performing foreign calls that will eventually try to 
+    // threads performing foreign calls that will eventually try to
     // return via resumeThread() and attempt to grab cap->lock.
     // closeMutex(&cap->lock);
 
@@ -940,7 +941,7 @@ rtsBool checkSparkCountInvariant (void)
         sparks.fizzled   += capabilities[i].spark_stats.fizzled;
         remaining        += sparkPoolSize(capabilities[i].sparks);
     }
-    
+
     /* The invariant is
      *   created = converted + remaining + gcd + fizzled
      */
diff --git a/rts/Schedule.c b/rts/Schedule.c
index 94c5e5f..2a48bc6 100644
--- a/rts/Schedule.c
+++ b/rts/Schedule.c
@@ -89,7 +89,7 @@ volatile StgWord recent_activity = ACTIVITY_YES;
  */
 volatile StgWord sched_state = SCHED_RUNNING;
 
-/*  This is used in `TSO.h' and gcc 2.96 insists that this variable actually 
+/*  This is used in `TSO.h' and gcc 2.96 insists that this variable actually
  *  exists - earlier gccs apparently didn't.
  *  -= chak
  */
@@ -170,8 +170,8 @@ static void deleteThread_(Capability *cap, StgTSO *tso);
 
    GRAN version:
      In a GranSim setup this loop iterates over the global event queue.
-     This revolves around the global event queue, which determines what 
-     to do next. Therefore, it's more complicated than either the 
+     This revolves around the global event queue, which determines what
+     to do next. Therefore, it's more complicated than either the
      concurrent or the parallel (GUM) setup.
   This version has been entirely removed (JB 2008/08).
 
@@ -180,7 +180,7 @@ static void deleteThread_(Capability *cap, StgTSO *tso);
      It starts with nothing to do (thus CurrentTSO == END_TSO_QUEUE),
      and sends out a fish whenever it has nothing to do; in-between
      doing the actual reductions (shared code below) it processes the
-     incoming messages and deals with delayed operations 
+     incoming messages and deals with delayed operations
      (see PendingFetches).
      This is not the ugliest code you could imagine, but it's bloody close.
 
@@ -203,7 +203,7 @@ schedule (Capability *initialCapability, Task *task)
 #if defined(THREADED_RTS)
   rtsBool first = rtsTrue;
 #endif
-  
+
   cap = initialCapability;
 
   // Pre-condition: this task owns initialCapability.
@@ -223,13 +223,13 @@ schedule (Capability *initialCapability, Task *task)
     // going via suspendThread()/resumeThread (i.e. a 'safe' foreign
     // call).
     if (cap->in_haskell) {
-    	  errorBelch("schedule: re-entered unsafely.\n"
-    		     "   Perhaps a 'foreign import unsafe' should be 'safe'?");
-    	  stg_exit(EXIT_FAILURE);
+          errorBelch("schedule: re-entered unsafely.\n"
+                     "   Perhaps a 'foreign import unsafe' should be 'safe'?");
+          stg_exit(EXIT_FAILURE);
     }
 
     // The interruption / shutdown sequence.
-    // 
+    //
     // In order to cleanly shut down the runtime, we want to:
     //   * make sure that all main threads return to their callers
     //     with the state 'Interrupted'.
@@ -247,7 +247,7 @@ schedule (Capability *initialCapability, Task *task)
     //     done by scheduleDoGC() for convenience (because GC already
     //     needs to acquire all the capabilities).  We can't kill
     //     threads involved in foreign calls.
-    // 
+    //
     //   * somebody calls shutdownHaskell(), which calls exitScheduler()
     //
     //   * sched_state := SCHED_SHUTTING_DOWN
@@ -259,9 +259,9 @@ schedule (Capability *initialCapability, Task *task)
     //   * eventually all Capabilities will shut down, and the RTS can
     //     exit.
     //
-    //   * We might be left with threads blocked in foreign calls, 
+    //   * We might be left with threads blocked in foreign calls,
     //     we should really attempt to kill these somehow (TODO);
-    
+
     switch (sched_state) {
     case SCHED_RUNNING:
 	break;
@@ -306,16 +306,16 @@ schedule (Capability *initialCapability, Task *task)
 #endif
 
     // Normally, the only way we can get here with no threads to
-    // run is if a keyboard interrupt received during 
+    // run is if a keyboard interrupt received during
     // scheduleCheckBlockedThreads() or scheduleDetectDeadlock().
     // Additionally, it is not fatal for the
     // threaded RTS to reach here with no threads to run.
     //
     // win32: might be here due to awaitEvent() being abandoned
     // as a result of a console event having been delivered.
-    
+
 #if defined(THREADED_RTS)
-    if (first) 
+    if (first)
     {
     // XXX: ToDo
     //     // don't yield the first time, we want a chance to run this
@@ -336,7 +336,7 @@ schedule (Capability *initialCapability, Task *task)
     }
 #endif
 
-    // 
+    //
     // Get a thread to run
     //
     t = popRunQueue(cap);
@@ -350,7 +350,7 @@ schedule (Capability *initialCapability, Task *task)
     // If not, we have to pass our capability to the right task.
     {
         InCall *bound = t->bound;
-      
+
 	if (bound) {
 	    if (bound->task == task) {
 		// yes, the Haskell thread is bound to the current native thread
@@ -364,14 +364,14 @@ schedule (Capability *initialCapability, Task *task)
 	    }
 	} else {
 	    // The thread we want to run is unbound.
-	    if (task->incall->tso) { 
+	    if (task->incall->tso) {
 		debugTrace(DEBUG_sched,
 			   "this OS thread cannot run thread %lu",
                            (unsigned long)t->id);
 		// no, the current native thread is bound to a different
 		// Haskell thread, so pass it to any worker thread
 		pushOnRunQueue(cap,t);
-		continue; 
+		continue;
 	    }
 	}
     }
@@ -394,7 +394,7 @@ schedule (Capability *initialCapability, Task *task)
 	&& !emptyThreadQueues(cap)) {
 	cap->context_switch = 1;
     }
-	 
+
 run_thread:
 
     // CurrentTSO is the thread to run.  t might be different if we
@@ -405,7 +405,7 @@ run_thread:
     startHeapProfTimer();
 
     // ----------------------------------------------------------------------
-    // Run the current thread 
+    // Run the current thread
 
     ASSERT_FULL_CAPABILITY_INVARIANTS(cap,task);
     ASSERT(t->cap == cap);
@@ -444,13 +444,13 @@ run_thread:
     traceEventRunThread(cap, t);
 
     switch (prev_what_next) {
-	
+
     case ThreadKilled:
     case ThreadComplete:
 	/* Thread already finished, return to scheduler. */
 	ret = ThreadFinished;
 	break;
-	
+
     case ThreadRunGHC:
     {
 	StgRegTable *r;
@@ -459,12 +459,12 @@ run_thread:
 	ret = r->rRet;
 	break;
     }
-    
+
     case ThreadInterpret:
 	cap = interpretBCO(cap);
 	ret = cap->r.rRet;
 	break;
-	
+
     default:
 	barf("schedule: invalid what_next field");
     }
@@ -500,13 +500,13 @@ run_thread:
     ASSERT(t->cap == cap);
 
     // ----------------------------------------------------------------------
-    
+
     // Costs for the scheduler are assigned to CCS_SYSTEM
     stopHeapProfTimer();
 #if defined(PROFILING)
     CCCS = CCS_SYSTEM;
 #endif
-    
+
     schedulePostRunThread(cap,t);
 
     ready_to_gc = rtsFalse;
@@ -526,7 +526,7 @@ run_thread:
     case ThreadYielding:
 	if (scheduleHandleYield(cap, t, prev_what_next)) {
             // shortcut for switching between compiler/interpreter:
-	    goto run_thread; 
+	    goto run_thread;
 	}
 	break;
 
@@ -580,7 +580,7 @@ removeFromRunQueue (Capability *cap, StgTSO *tso)
 static void
 schedulePreLoop(void)
 {
-  // initialisation for scheduler - what cannot go into initScheduler()  
+  // initialisation for scheduler - what cannot go into initScheduler()
 }
 
 /* -----------------------------------------------------------------------------
@@ -613,7 +613,7 @@ shouldYieldCapability (Capability *cap, Task *task)
     //   - the thread at the head of the run queue cannot be run
     //     by this Task (it is bound to another Task, or it is unbound
     //     and this task it bound).
-    return (waiting_for_gc || 
+    return (waiting_for_gc ||
             cap->returning_tasks_hd != NULL ||
             (!emptyRunQueue(cap) && (task->incall->tso == NULL
                                      ? cap->run_queue_hd->bound != NULL
@@ -623,7 +623,7 @@ shouldYieldCapability (Capability *cap, Task *task)
 // This is the single place where a Task goes to sleep.  There are
 // two reasons it might need to sleep:
 //    - there are no threads to run
-//    - we need to yield this Capability to someone else 
+//    - we need to yield this Capability to someone else
 //      (see shouldYieldCapability())
 //
 // Careful: the scheduler loop is quite delicate.  Make sure you run
@@ -637,7 +637,7 @@ scheduleYield (Capability **pcap, Task *task)
 
     // if we have work, and we don't need to give up the Capability, continue.
     //
-    if (!shouldYieldCapability(cap,task) && 
+    if (!shouldYieldCapability(cap,task) &&
         (!emptyRunQueue(cap) ||
          !emptyInbox(cap) ||
          sched_state >= SCHED_INTERRUPTING))
@@ -646,7 +646,7 @@ scheduleYield (Capability **pcap, Task *task)
     // otherwise yield (sleep), and keep yielding if necessary.
     do {
         yieldCapability(&cap,task);
-    } 
+    }
     while (shouldYieldCapability(cap,task));
 
     // note there may still be no threads on the run queue at this
@@ -656,7 +656,7 @@ scheduleYield (Capability **pcap, Task *task)
     return;
 }
 #endif
-    
+
 /* -----------------------------------------------------------------------------
  * schedulePushWork()
  *
@@ -664,7 +664,7 @@ scheduleYield (Capability **pcap, Task *task)
  * -------------------------------------------------------------------------- */
 
 static void
-schedulePushWork(Capability *cap USED_IF_THREADS, 
+schedulePushWork(Capability *cap USED_IF_THREADS,
 		 Task *task      USED_IF_THREADS)
 {
   /* following code not for PARALLEL_HASKELL. I kept the call general,
@@ -693,7 +693,7 @@ schedulePushWork(Capability *cap USED_IF_THREADS,
 	    if (!emptyRunQueue(cap0)
                 || cap->returning_tasks_hd != NULL
                 || cap->inbox != (Message*)END_TSO_QUEUE) {
-		// it already has some work, we just grabbed it at 
+		// it already has some work, we just grabbed it at
 		// the wrong moment.  Or maybe it's deadlocked!
 		releaseCapability(cap0);
 	    } else {
@@ -707,7 +707,7 @@ schedulePushWork(Capability *cap USED_IF_THREADS,
     // probably the simplest thing we could do; improvements we might
     // want to do include:
     //
-    //   - giving high priority to moving relatively new threads, on 
+    //   - giving high priority to moving relatively new threads, on
     //     the gournds that they haven't had time to build up a
     //     working set in the cache on this CPU/Capability.
     //
@@ -719,9 +719,9 @@ schedulePushWork(Capability *cap USED_IF_THREADS,
 	rtsBool pushed_to_all;
 #endif
 
-	debugTrace(DEBUG_sched, 
-		   "cap %d: %s and %d free capabilities, sharing...", 
-		   cap->no, 
+	debugTrace(DEBUG_sched,
+		   "cap %d: %s and %d free capabilities, sharing...",
+		   cap->no,
 		   (!emptyRunQueue(cap) && cap->run_queue_hd->_link != END_TSO_QUEUE)?
 		   "excess threads on run queue":"sparks to share (>=2)",
 		   n_free_caps);
@@ -853,7 +853,7 @@ scheduleCheckBlockedThreads(Capability *cap USED_IF_NOT_THREADS)
 static void
 scheduleDetectDeadlock (Capability *cap, Task *task)
 {
-    /* 
+    /*
      * Detect deadlock: when we have no threads to run, there are no
      * threads blocked, waiting for I/O, or sleeping, and all the
      * other tasks are waiting for work, we must have a deadlock of
@@ -862,7 +862,7 @@ scheduleDetectDeadlock (Capability *cap, Task *task)
     if ( emptyThreadQueues(cap) )
     {
 #if defined(THREADED_RTS)
-	/* 
+	/*
 	 * In the threaded RTS, we only check for deadlock if there
 	 * has been no activity in a complete timeslice.  This means
 	 * we won't eagerly start a full GC just because we don't have
@@ -917,7 +917,7 @@ scheduleDetectDeadlock (Capability *cap, Task *task)
 	    case BlockedOnBlackHole:
 	    case BlockedOnMsgThrowTo:
 	    case BlockedOnMVar:
-		throwToSingleThreaded(cap, task->incall->tso, 
+		throwToSingleThreaded(cap, task->incall->tso,
 				      (StgClosure *)nonTermination_closure);
 		return;
 	    default:
@@ -944,11 +944,11 @@ scheduleSendPendingMessages(void)
         processFetches();
     }
 # endif
-    
+
     if (RtsFlags.ParFlags.BufferTime) {
 	// if we use message buffering, we must send away all message
 	// packets which have become too old...
-	sendOldBuffers(); 
+	sendOldBuffers();
     }
 }
 #endif
@@ -1022,7 +1022,7 @@ schedulePostRunThread (Capability *cap, StgTSO *t)
 {
     // We have to be able to catch transactions that are in an
     // infinite loop as a result of seeing an inconsistent view of
-    // memory, e.g. 
+    // memory, e.g.
     //
     //   atomically $ do
     //       [a,b] <- mapM readTVar [ta,tb]
@@ -1034,13 +1034,13 @@ schedulePostRunThread (Capability *cap, StgTSO *t)
         if (!stmValidateNestOfTransactions (t -> trec)) {
             debugTrace(DEBUG_sched | DEBUG_stm,
                        "trec %p found wasting its time", t);
-            
+
             // strip the stack back to the
             // ATOMICALLY_FRAME, aborting the (nested)
             // transaction, and saving the stack of any
             // partially-evaluated thunks on the heap.
             throwToSingleThreaded_(cap, t, NULL, rtsTrue);
-            
+
 //            ASSERT(get_itbl((StgClosure *)t->sp)->type == ATOMICALLY_FRAME);
         }
     }
@@ -1060,25 +1060,25 @@ scheduleHandleHeapOverflow( Capability *cap, StgTSO *t )
 	// if so, get one and push it on the front of the nursery.
 	bdescr *bd;
 	lnat blocks;
-	
+
 	blocks = (lnat)BLOCK_ROUND_UP(cap->r.rHpAlloc) / BLOCK_SIZE;
-	
+
         if (blocks > BLOCKS_PER_MBLOCK) {
             barf("allocation of %ld bytes too large (GHC should have complained at compile-time)", (long)cap->r.rHpAlloc);
         }
 
 	debugTrace(DEBUG_sched,
-		   "--<< thread %ld (%s) stopped: requesting a large block (size %ld)\n", 
+		   "--<< thread %ld (%s) stopped: requesting a large block (size %ld)\n",
 		   (long)t->id, what_next_strs[t->what_next], blocks);
-    
+
 	// don't do this if the nursery is (nearly) full, we'll GC first.
 	if (cap->r.rCurrentNursery->link != NULL ||
 	    cap->r.rNursery->n_blocks == 1) {  // paranoia to prevent infinite loop
 	                                       // if the nursery has only one block.
-	    
+
             bd = allocGroup_lock(blocks);
             cap->r.rNursery->n_blocks += blocks;
-	    
+
 	    // link the new group into the list
 	    bd->link = cap->r.rCurrentNursery;
 	    bd->u.back = cap->r.rCurrentNursery->u.back;
@@ -1086,9 +1086,9 @@ scheduleHandleHeapOverflow( Capability *cap, StgTSO *t )
 		cap->r.rCurrentNursery->u.back->link = bd;
 	    } else {
 		cap->r.rNursery->blocks = bd;
-	    }		  
+	    }
 	    cap->r.rCurrentNursery->u.back = bd;
-	    
+
 	    // initialise it as a nursery block.  We initialise the
 	    // step, gen_no, and flags field of *every* sub-block in
 	    // this large block, because this is easier than making
@@ -1096,7 +1096,7 @@ scheduleHandleHeapOverflow( Capability *cap, StgTSO *t )
 	    // block whenever we call Bdescr() (eg. evacuate() and
 	    // isAlive() in the GC would both have to do this, at
 	    // least).
-	    { 
+	    {
 		bdescr *x;
 		for (x = bd; x < bd + blocks; x++) {
                     initBdescr(x,g0,g0);
@@ -1104,14 +1104,14 @@ scheduleHandleHeapOverflow( Capability *cap, StgTSO *t )
 		    x->flags = 0;
 		}
 	    }
-	    
+
 	    // This assert can be a killer if the app is doing lots
 	    // of large block allocations.
 	    IF_DEBUG(sanity, checkNurserySanity(cap->r.rNursery));
-	    
+
 	    // now update the nursery to point to the new block
 	    cap->r.rCurrentNursery = bd;
-	    
+
 	    // we might be unlucky and have another thread get on the
 	    // run queue before us and steal the large block, but in that
 	    // case the thread will just end up requesting another large
@@ -1120,7 +1120,7 @@ scheduleHandleHeapOverflow( Capability *cap, StgTSO *t )
 	    return rtsFalse;  /* not actually GC'ing */
 	}
     }
-    
+
     if (cap->r.rHpLim == NULL || cap->context_switch) {
         // Sometimes we miss a context switch, e.g. when calling
         // primitives in a tight loop, MAYBE_GC() doesn't check the
@@ -1149,13 +1149,13 @@ scheduleHandleYield( Capability *cap, StgTSO *t, nat prev_what_next )
      */
 
     ASSERT(t->_link == END_TSO_QUEUE);
-    
+
     // Shortcut if we're just switching evaluators: don't bother
     // doing stack squeezing (which can be expensive), just run the
     // thread.
     if (cap->context_switch == 0 && t->what_next != prev_what_next) {
 	debugTrace(DEBUG_sched,
-		   "--<< thread %ld (%s) stopped to switch evaluators", 
+		   "--<< thread %ld (%s) stopped to switch evaluators",
 		   (long)t->id, what_next_strs[t->what_next]);
 	return rtsTrue;
     }
@@ -1167,7 +1167,7 @@ scheduleHandleYield( Capability *cap, StgTSO *t, nat prev_what_next )
     // penalises threads that do a lot of allocation, but that seems
     // better than the alternative.
     cap->context_switch = 0;
-    
+
     IF_DEBUG(sanity,
 	     //debugBelch("&& Doing sanity check on yielding TSO %ld.", t->id);
 	     checkTSO(t));
@@ -1224,7 +1224,7 @@ scheduleHandleThreadFinished (Capability *cap STG_UNUSED, Task *task, StgTSO *t)
 
       //
       // Check whether the thread that just completed was a bound
-      // thread, and if so return with the result.  
+      // thread, and if so return with the result.
       //
       // There is an assumption here that all thread completion goes
       // through this point; we need to make sure that if a thread
@@ -1320,7 +1320,7 @@ scheduleDoGC (Capability *cap, Task *task USED_IF_THREADS, rtsBool force_major)
 {
     rtsBool heap_census;
 #ifdef THREADED_RTS
-    /* extern static volatile StgWord waiting_for_gc; 
+    /* extern static volatile StgWord waiting_for_gc;
        lives inside capability.c */
     rtsBool gc_type, prev_pending_gc;
     nat i;
@@ -1346,7 +1346,7 @@ scheduleDoGC (Capability *cap, Task *task USED_IF_THREADS, rtsBool force_major)
 
     // In order to GC, there must be no threads running Haskell code.
     // Therefore, the GC thread needs to hold *all* the capabilities,
-    // and release them after the GC has completed.  
+    // and release them after the GC has completed.
     //
     // This seems to be the simplest way: previous attempts involved
     // making all the threads with capabilities give up their
@@ -1362,7 +1362,7 @@ scheduleDoGC (Capability *cap, Task *task USED_IF_THREADS, rtsBool force_major)
     prev_pending_gc = cas(&waiting_for_gc, 0, gc_type);
     if (prev_pending_gc) {
 	do {
-	    debugTrace(DEBUG_sched, "someone else is trying to GC (%d)...", 
+	    debugTrace(DEBUG_sched, "someone else is trying to GC (%d)...",
                        prev_pending_gc);
             ASSERT(cap);
             yieldCapability(&cap,task);
@@ -1374,7 +1374,7 @@ scheduleDoGC (Capability *cap, Task *task USED_IF_THREADS, rtsBool force_major)
 
     // The final shutdown GC is always single-threaded, because it's
     // possible that some of the Capabilities have no worker threads.
-    
+
     if (gc_type == PENDING_GC_SEQ)
     {
         traceEventRequestSeqGc(cap);
@@ -1410,7 +1410,7 @@ scheduleDoGC (Capability *cap, Task *task USED_IF_THREADS, rtsBool force_major)
         // multi-threaded GC: make sure all the Capabilities donate one
         // GC thread each.
         waitForGcThreads(cap);
-        
+
 #if defined(THREADED_RTS)
         // Stable point where we can do a global check on our spark counters
         ASSERT(checkSparkCountInvariant());
@@ -1431,7 +1431,7 @@ delete_threads_and_gc:
 	deleteAllThreads(cap);
 	sched_state = SCHED_SHUTTING_DOWN;
     }
-    
+
     heap_census = scheduleNeedHeapProfile(rtsTrue);
 
     traceEventGcStart(cap);
@@ -1500,7 +1500,7 @@ delete_threads_and_gc:
     }
 
 #ifdef SPARKBALANCE
-    /* JB 
+    /* JB
        Once we are all together... this would be the place to balance all
        spark pools. No concurrent stealing or adding of new sparks can
        occur. Should be defined in Sparks.c. */
@@ -1543,7 +1543,7 @@ forkProcess(HsStablePtr *entry
     StgTSO* t,*next;
     Capability *cap;
     nat g;
-    
+
 #if defined(THREADED_RTS)
     if (RtsFlags.ParFlags.nNodes > 1) {
 	errorBelch("forking not supported with +RTS -N<n> greater than 1");
@@ -1552,10 +1552,10 @@ forkProcess(HsStablePtr *entry
 #endif
 
     debugTrace(DEBUG_sched, "forking!");
-    
+
     // ToDo: for SMP, we should probably acquire *all* the capabilities
     cap = rts_lock();
-    
+
     // no funny business: hold locks while we fork, otherwise if some
     // other thread is holding a lock when the fork happens, the data
     // structure protected by the lock will forever be in an
@@ -1571,9 +1571,9 @@ forkProcess(HsStablePtr *entry
 #endif
 
     pid = fork();
-    
+
     if (pid) { // parent
-	
+
         startTimer(); // #4074
 
         RELEASE_LOCK(&sched_mutex);
@@ -1583,9 +1583,9 @@ forkProcess(HsStablePtr *entry
 	// just return the pid
 	rts_unlock(cap);
 	return pid;
-	
+
     } else { // child
-	
+
 #if defined(THREADED_RTS)
         initMutex(&sched_mutex);
         initMutex(&cap->lock);
@@ -1618,7 +1618,7 @@ forkProcess(HsStablePtr *entry
                 t->bound = NULL;
           }
 	}
-	
+
 	// Empty the run queue.  It seems tempting to let all the
 	// killed threads stay on the run queue as zombies to be
 	// cleaned up later, but some of them correspond to bound
@@ -1658,7 +1658,7 @@ forkProcess(HsStablePtr *entry
 
 	cap = rts_evalStableIO(cap, entry, NULL);  // run the action
 	rts_checkSchedStatus("forkProcess",cap);
-	
+
 	rts_unlock(cap);
 	hs_exit();                      // clean up and exit
 	stg_exit(EXIT_SUCCESS);
@@ -1671,7 +1671,7 @@ forkProcess(HsStablePtr *entry
 /* ---------------------------------------------------------------------------
  * Delete all the threads in the system
  * ------------------------------------------------------------------------- */
-   
+
 static void
 deleteAllThreads ( Capability *cap )
 {
@@ -1709,7 +1709,7 @@ STATIC_INLINE void
 suspendTask (Capability *cap, Task *task)
 {
     InCall *incall;
-    
+
     incall = task->incall;
     ASSERT(incall->next == NULL && incall->prev == NULL);
     incall->next = cap->suspended_ccalls;
@@ -1740,7 +1740,7 @@ recoverSuspendedTask (Capability *cap, Task *task)
 
 /* ---------------------------------------------------------------------------
  * Suspending & resuming Haskell threads.
- * 
+ *
  * When making a "safe" call to C (aka _ccall_GC), the task gives back
  * its capability before calling the C function.  This allows another
  * task to pick up the capability and carry on running Haskell
@@ -1756,7 +1756,7 @@ recoverSuspendedTask (Capability *cap, Task *task)
  * unceremoniously terminated and should be scheduled on an
  * unbound worker thread.
  * ------------------------------------------------------------------------- */
-   
+
 void *
 suspendThread (StgRegTable *reg, rtsBool interruptible)
 {
@@ -1802,7 +1802,7 @@ suspendThread (StgRegTable *reg, rtsBool interruptible)
   suspendTask(cap,task);
   cap->in_haskell = rtsFalse;
   releaseCapability_(cap,rtsFalse);
-  
+
   RELEASE_LOCK(&cap->lock);
 
   errno = saved_errno;
@@ -1848,7 +1848,7 @@ resumeThread (void *task_)
     tso->_link = END_TSO_QUEUE; // no write barrier reqd
 
     traceEventRunThread(cap, tso);
-    
+
     /* Reset blocking status */
     tso->why_blocked  = NotBlocked;
 
@@ -1858,7 +1858,7 @@ resumeThread (void *task_)
             maybePerformBlockedException(cap,tso);
         }
     }
-    
+
     cap->r.rCurrentTSO = tso;
     cap->in_haskell = rtsTrue;
     errno = saved_errno;
@@ -1980,7 +1980,7 @@ void scheduleWorker (Capability *cap, Task *task)
  *
  * ------------------------------------------------------------------------ */
 
-void 
+void
 initScheduler(void)
 {
 #if !defined(THREADED_RTS)
@@ -1997,7 +1997,7 @@ initScheduler(void)
    * the scheduler. */
   initMutex(&sched_mutex);
 #endif
-  
+
   ACQUIRE_LOCK(&sched_mutex);
 
   /* A capability holds the state a native thread needs in
@@ -2017,7 +2017,7 @@ initScheduler(void)
    * bound thread on Capability 0 pretty soon, so we don't want a
    * worker task hogging it.
    */
-  { 
+  {
       nat i;
       Capability *cap;
       for (i = 1; i < n_capabilities; i++) {
@@ -2078,14 +2078,14 @@ freeScheduler( void )
 #endif
 }
 
-void markScheduler (evac_fn evac USED_IF_NOT_THREADS, 
+void markScheduler (evac_fn evac USED_IF_NOT_THREADS,
                     void *user USED_IF_NOT_THREADS)
 {
 #if !defined(THREADED_RTS)
     evac(user, (StgClosure **)(void *)&blocked_queue_hd);
     evac(user, (StgClosure **)(void *)&blocked_queue_tl);
     evac(user, (StgClosure **)(void *)&sleeping_queue);
-#endif 
+#endif
 }
 
 /* -----------------------------------------------------------------------------
@@ -2102,7 +2102,7 @@ performGC_(rtsBool force_major)
     Task *task;
 
     // We must grab a new Task here, because the existing Task may be
-    // associated with a particular Capability, and chained onto the 
+    // associated with a particular Capability, and chained onto the
     // suspended_ccalls queue.
     task = newBoundTask();
 
@@ -2126,7 +2126,7 @@ performMajorGC(void)
 
 /* ---------------------------------------------------------------------------
    Interrupt execution
-   - usually called inside a signal handler so it mustn't do anything fancy.   
+   - usually called inside a signal handler so it mustn't do anything fancy.
    ------------------------------------------------------------------------ */
 
 void
@@ -2141,7 +2141,7 @@ interruptStgRts(void)
 
 /* -----------------------------------------------------------------------------
    Wake up the RTS
-   
+
    This function causes at least one OS thread to wake up and run the
    scheduler loop.  It is invoked when the RTS might be deadlocked, or
    an external event has arrived that may need servicing (eg. a
@@ -2175,7 +2175,7 @@ deleteThread (Capability *cap STG_UNUSED, StgTSO *tso)
 {
     // NOTE: must only be called on a TSO that we have exclusive
     // access to, because we will call throwToSingleThreaded() below.
-    // The TSO must be on the run queue of the Capability we own, or 
+    // The TSO must be on the run queue of the Capability we own, or
     // we must own all Capabilities.
 
     if (tso->why_blocked != BlockedOnCCall &&
@@ -2202,7 +2202,7 @@ deleteThread_(Capability *cap, StgTSO *tso)
 
 /* -----------------------------------------------------------------------------
    raiseExceptionHelper
-   
+
    This function is called by the raise# primitve, just so that we can
    move some of the tricky bits of raising an exception from C-- into
    C.  Who knows, it might be a useful re-useable thing here too.
@@ -2231,7 +2231,7 @@ raiseExceptionHelper (StgRegTable *reg, StgTSO *tso, StgClosure *exception)
     // use MIN_UPD_SIZE.
     //
     // raise_closure = (StgClosure *)RET_STGCALL1(P_,allocate,
-    // 				       sizeofW(StgClosure)+1);
+    //                                 sizeofW(StgClosure)+1);
     //
 
     //
@@ -2244,11 +2244,11 @@ raiseExceptionHelper (StgRegTable *reg, StgTSO *tso, StgClosure *exception)
 	info = get_ret_itbl((StgClosure *)p);
 	next = p + stack_frame_sizeW((StgClosure *)p);
 	switch (info->i.type) {
-	    
+
 	case UPDATE_FRAME:
 	    // Only create raise_closure if we need to.
 	    if (raise_closure == NULL) {
-		raise_closure = 
+		raise_closure =
 		    (StgThunk *)allocate(cap,sizeofW(StgThunk)+1);
 		SET_HDR(raise_closure, &stg_raise_info, CCCS);
 		raise_closure->payload[0] = exception;
@@ -2262,7 +2262,7 @@ raiseExceptionHelper (StgRegTable *reg, StgTSO *tso, StgClosure *exception)
 	    debugTrace(DEBUG_stm, "found ATOMICALLY_FRAME at %p", p);
             tso->stackobj->sp = p;
             return ATOMICALLY_FRAME;
-	    
+
 	case CATCH_FRAME:
             tso->stackobj->sp = p;
 	    return CATCH_FRAME;
@@ -2271,7 +2271,7 @@ raiseExceptionHelper (StgRegTable *reg, StgTSO *tso, StgClosure *exception)
 	    debugTrace(DEBUG_stm, "found CATCH_STM_FRAME at %p", p);
             tso->stackobj->sp = p;
             return CATCH_STM_FRAME;
-	    
+
         case UNDERFLOW_FRAME:
             tso->stackobj->sp = p;
             threadStackUnderflow(cap,tso);
@@ -2284,7 +2284,7 @@ raiseExceptionHelper (StgRegTable *reg, StgTSO *tso, StgClosure *exception)
 
         case CATCH_RETRY_FRAME:
 	default:
-	    p = next; 
+	    p = next;
 	    continue;
 	}
     }
@@ -2295,10 +2295,10 @@ raiseExceptionHelper (StgRegTable *reg, StgTSO *tso, StgClosure *exception)
    findRetryFrameHelper
 
    This function is called by the retry# primitive.  It traverses the stack
-   leaving tso->sp referring to the frame which should handle the retry.  
+   leaving tso->sp referring to the frame which should handle the retry.
 
-   This should either be a CATCH_RETRY_FRAME (if the retry# is within an orElse#) 
-   or should be a ATOMICALLY_FRAME (if the retry# reaches the top level).  
+   This should either be a CATCH_RETRY_FRAME (if the retry# is within an orElse#)
+   or should be a ATOMICALLY_FRAME (if the retry# reaches the top level).
 
    We skip CATCH_STM_FRAMEs (aborting and rolling back the nested tx that they
    create) because retries are not considered to be exceptions, despite the
@@ -2319,19 +2319,19 @@ findRetryFrameHelper (Capability *cap, StgTSO *tso)
     info = get_ret_itbl((StgClosure *)p);
     next = p + stack_frame_sizeW((StgClosure *)p);
     switch (info->i.type) {
-      
+
     case ATOMICALLY_FRAME:
 	debugTrace(DEBUG_stm,
 		   "found ATOMICALLY_FRAME at %p during retry", p);
         tso->stackobj->sp = p;
 	return ATOMICALLY_FRAME;
-      
+
     case CATCH_RETRY_FRAME:
 	debugTrace(DEBUG_stm,
 		   "found CATCH_RETRY_FRAME at %p during retrry", p);
         tso->stackobj->sp = p;
 	return CATCH_RETRY_FRAME;
-      
+
     case CATCH_STM_FRAME: {
         StgTRecHeader *trec = tso -> trec;
 	StgTRecHeader *outer = trec -> enclosing_trec;
@@ -2341,10 +2341,10 @@ findRetryFrameHelper (Capability *cap, StgTSO *tso)
         stmAbortTransaction(cap, trec);
         stmFreeAbortedTRec(cap, trec);
 	tso -> trec = outer;
-        p = next; 
+        p = next;
         continue;
     }
-      
+
     case UNDERFLOW_FRAME:
         threadStackUnderflow(cap,tso);
         p = tso->stackobj->sp;
@@ -2353,7 +2353,7 @@ findRetryFrameHelper (Capability *cap, StgTSO *tso)
     default:
       ASSERT(info->i.type != CATCH_FRAME);
       ASSERT(info->i.type != STOP_FRAME);
-      p = next; 
+      p = next;
       continue;
     }
   }
@@ -2384,10 +2384,10 @@ resurrectThreads (StgTSO *threads)
 	gen->threads = tso;
 
 	debugTrace(DEBUG_sched, "resurrecting thread %lu", (unsigned long)tso->id);
-	
+
 	// Wake up the thread on the Capability it was last on
 	cap = tso->cap;
-	
+
 	switch (tso->why_blocked) {
 	case BlockedOnMVar:
 	    /* Called by GC - sched_mutex lock is currently held. */
