From c93fdc2529e18b4eca04e1bf94640a115be49893 Mon Sep 17 00:00:00 2001
From: Mikolaj <mikolaj.konarski@gmail.com>
Date: Fri, 8 Jul 2011 14:04:18 +0200
Subject: [PATCH 3/3] fix whitespace (mostly trailing and tabs) in sparklog-related files

---
 includes/rts/EventLogFormat.h |    8 +-
 includes/rts/Flags.h          |   10 +-
 rts/Capability.c              |   30 +++---
 rts/Capability.h              |   14 ++--
 rts/RtsFlags.c                |   48 +++++-----
 rts/Schedule.c                |  230 ++++++++++++++++++++--------------------
 rts/Schedule.h                |    9 +-
 rts/Trace.c                   |   28 +++---
 rts/Trace.h                   |   40 ++++----
 rts/eventlog/EventLog.c       |   48 +++++-----
 rts/eventlog/EventLog.h       |   10 +-
 11 files changed, 237 insertions(+), 238 deletions(-)

diff --git a/includes/rts/EventLogFormat.h b/includes/rts/EventLogFormat.h
index 1ea8d67..e4c6c85 100644
--- a/includes/rts/EventLogFormat.h
+++ b/includes/rts/EventLogFormat.h
@@ -3,16 +3,16 @@
  * (c) The GHC Team, 2008-2009
  *
  * Event log format
- * 
+ *
  * The log format is designed to be extensible: old tools should be
  * able to parse (but not necessarily understand all of) new versions
  * of the format, and new tools will be able to understand old log
  * files.
- * 
+ *
  * Each event has a specific format.  If you add new events, give them
  * new numbers: we never re-use old event numbers.
  *
- * - The format is endian-independent: all values are represented in 
+ * - The format is endian-independent: all values are represented in
  *    bigendian order.
  *
  * - The format is extensible:
@@ -51,7 +51,7 @@
  *       Word8*         -- extra info (for future extensions)
  *       EVENT_ET_END
  *
- * Event : 
+ * Event :
  *       Word16         -- event_type
  *       Word64         -- time (nanosecs)
  *       [Word16]       -- length of the rest (for variable-sized events only)
diff --git a/includes/rts/Flags.h b/includes/rts/Flags.h
index 63dc72e..4ed7aef 100644
--- a/includes/rts/Flags.h
+++ b/includes/rts/Flags.h
@@ -57,7 +57,7 @@ struct GC_FLAGS {
     StgWord heapBase;           /* address to ask the OS for memory */
 };
 
-struct DEBUG_FLAGS {  
+struct DEBUG_FLAGS {
     /* flags to control debugging output & extra checking in various subsystems */
     rtsBool scheduler;      /* 's' */
     rtsBool interpreter;    /* 'i' */
@@ -72,8 +72,8 @@ struct DEBUG_FLAGS {
     rtsBool apply;          /* 'a' */
     rtsBool stm;            /* 'm' */
     rtsBool squeeze;        /* 'z'  stack squeezing & lazy blackholing */
-    rtsBool hpc; 	    /* 'c' coverage */
-    rtsBool sparks; 	    /* 'r' */
+    rtsBool hpc;            /* 'c' coverage */
+    rtsBool sparks;         /* 'r' */
 };
 
 struct COST_CENTRE_FLAGS {
@@ -152,7 +152,7 @@ struct PAR_FLAGS {
   rtsBool        parGcEnabled;   /* enable parallel GC */
   unsigned int   parGcGen;       /* do parallel GC in this generation
                                   * and higher only */
-  rtsBool        parGcLoadBalancingEnabled; 
+  rtsBool        parGcLoadBalancingEnabled;
                                  /* enable load-balancing in the
                                   * parallel GC */
   unsigned int   parGcLoadBalancingGen;
@@ -222,7 +222,7 @@ extern RTS_FLAGS RtsFlags;
 /*
  * The printf formats are here, so we are less likely to make
  * overly-long filenames (with disastrous results).  No more than 128
- * chars, please!  
+ * chars, please!
  */
 
 #define STATS_FILENAME_MAXLEN	128
diff --git a/rts/Capability.c b/rts/Capability.c
index e03a1f2..2d11ecc 100644
--- a/rts/Capability.c
+++ b/rts/Capability.c
@@ -112,7 +112,7 @@ findSpark (Capability *cap)
       if (n_capabilities == 1) { return NULL; } // makes no sense...
 
       debugTrace(DEBUG_sched,
-                 "cap %d: Trying to steal work from other capabilities", 
+                 "cap %d: Trying to steal work from other capabilities",
                  cap->no);
 
       /* visit cap.s 0..n-1 in sequence until a theft succeeds. We could
@@ -140,7 +140,7 @@ findSpark (Capability *cap)
               cap->spark_stats.converted++;
 
               traceEventStealSpark(cap, cap->r.rCurrentTSO, robbed->no);
-              
+
               return spark;
           }
           // otherwise: no success, try next one
@@ -281,8 +281,8 @@ initCapability( Capability *cap, nat i )
 void
 initCapabilities( void )
 {
-    /* Declare a single capability set representing the process. 
-       Each capability will get added to this capset. */ 
+    /* Declare a single capability set representing the process.
+       Each capability will get added to this capset. */
     traceCapsetCreate(CAPSET_OSPROCESS_DEFAULT, CapsetTypeOsProcess);
 
 #if defined(THREADED_RTS)
@@ -384,7 +384,7 @@ giveCapabilityToTask (Capability *cap USED_IF_DEBUG, Task *task)
 
 #if defined(THREADED_RTS)
 void
-releaseCapability_ (Capability* cap, 
+releaseCapability_ (Capability* cap,
                     rtsBool always_wakeup)
 {
     Task *task;
@@ -407,7 +407,7 @@ releaseCapability_ (Capability* cap,
       last_free_capability = cap; // needed?
       debugTrace(DEBUG_sched, "GC pending, set capability %d free", cap->no);
       return;
-    } 
+    }
 
 
     // If the next thread on the run queue is a bound thread,
@@ -438,7 +438,7 @@ releaseCapability_ (Capability* cap,
 
     // If we have an unbound thread on the run queue, or if there's
     // anything else to do, give the Capability to a worker thread.
-    if (always_wakeup || 
+    if (always_wakeup ||
         !emptyRunQueue(cap) || !emptyInbox(cap) ||
         !emptySparkPoolCap(cap) || globalWorkToDo()) {
 	if (cap->spare_workers) {
@@ -644,7 +644,7 @@ yieldCapability (Capability** pCap, Task *task)
 
 	    ACQUIRE_LOCK(&cap->lock);
 	    if (cap->running_task != NULL) {
-		debugTrace(DEBUG_sched, 
+		debugTrace(DEBUG_sched,
 			   "capability %d is owned by another task", cap->no);
 		RELEASE_LOCK(&cap->lock);
 		continue;
@@ -681,7 +681,7 @@ yieldCapability (Capability** pCap, Task *task)
 /* ----------------------------------------------------------------------------
  * prodCapability
  *
- * If a Capability is currently idle, wake up a Task on it.  Used to 
+ * If a Capability is currently idle, wake up a Task on it.  Used to
  * get every Capability into the GC.
  * ------------------------------------------------------------------------- */
 
@@ -755,7 +755,7 @@ shutdownCapability (Capability *cap,
     for (i = 0; /* i < 50 */; i++) {
         ASSERT(sched_state == SCHED_SHUTTING_DOWN);
 
-	debugTrace(DEBUG_sched, 
+	debugTrace(DEBUG_sched,
 		   "shutting down capability %d, attempt %d", cap->no, i);
 	ACQUIRE_LOCK(&cap->lock);
 	if (cap->running_task) {
@@ -777,7 +777,7 @@ shutdownCapability (Capability *cap,
             prev = NULL;
             for (t = cap->spare_workers; t != NULL; t = t->next) {
                 if (!osThreadIsAlive(t->id)) {
-                    debugTrace(DEBUG_sched, 
+                    debugTrace(DEBUG_sched,
                                "worker thread %p has died unexpectedly", (void *)t->id);
                     cap->n_spare_workers--;
                     if (!prev) {
@@ -791,7 +791,7 @@ shutdownCapability (Capability *cap,
         }
 
 	if (!emptyRunQueue(cap) || cap->spare_workers) {
-	    debugTrace(DEBUG_sched, 
+	    debugTrace(DEBUG_sched,
 		       "runnable threads or workers still alive, yielding");
 	    releaseCapability_(cap,rtsFalse); // this will wake up a worker
 	    RELEASE_LOCK(&cap->lock);
@@ -806,7 +806,7 @@ shutdownCapability (Capability *cap,
         // We can be a bit more relaxed when this is a standalone
         // program that is about to terminate, and let safe=false.
         if (cap->suspended_ccalls && safe) {
-	    debugTrace(DEBUG_sched, 
+	    debugTrace(DEBUG_sched,
 		       "thread(s) are involved in foreign calls, yielding");
             cap->running_task = NULL;
 	    RELEASE_LOCK(&cap->lock);
@@ -830,7 +830,7 @@ shutdownCapability (Capability *cap,
     // list are both empty.
 
     // ToDo: we can't drop this mutex, because there might still be
-    // threads performing foreign calls that will eventually try to 
+    // threads performing foreign calls that will eventually try to
     // return via resumeThread() and attempt to grab cap->lock.
     // closeMutex(&cap->lock);
 
@@ -941,7 +941,7 @@ rtsBool checkSparkCountInvariant (void)
         sparks.fizzled   += capabilities[i].spark_stats.fizzled;
         remaining        += sparkPoolSize(capabilities[i].sparks);
     }
-    
+
     /* The invariant is
      *   created = converted + remaining + gcd + fizzled
      */
diff --git a/rts/Capability.h b/rts/Capability.h
index 10c7c49..3eeeacd 100644
--- a/rts/Capability.h
+++ b/rts/Capability.h
@@ -72,7 +72,7 @@ struct Capability_ {
     // block for allocating pinned objects into
     bdescr *pinned_object_block;
 
-    // Context switch flag. We used to have one global flag, now one 
+    // Context switch flag. We used to have one global flag, now one
     // per capability. Locks required  : none (conflicts are harmless)
     int context_switch;
 
@@ -164,18 +164,18 @@ void initCapabilities (void);
 #if defined(THREADED_RTS)
 void releaseCapability           (Capability* cap);
 void releaseAndWakeupCapability  (Capability* cap);
-void releaseCapability_ (Capability* cap, rtsBool always_wakeup); 
+void releaseCapability_ (Capability* cap, rtsBool always_wakeup);
 // assumes cap->lock is held
 #else
 // releaseCapability() is empty in non-threaded RTS
 INLINE_HEADER void releaseCapability  (Capability* cap STG_UNUSED) {};
 INLINE_HEADER void releaseAndWakeupCapability  (Capability* cap STG_UNUSED) {};
-INLINE_HEADER void releaseCapability_ (Capability* cap STG_UNUSED, 
+INLINE_HEADER void releaseCapability_ (Capability* cap STG_UNUSED,
                                        rtsBool always_wakeup STG_UNUSED) {};
 #endif
 
 // declared in includes/rts/Threads.h:
-// extern Capability MainCapability; 
+// extern Capability MainCapability;
 
 // declared in includes/rts/Threads.h:
 // extern nat n_capabilities;
@@ -333,15 +333,15 @@ recordClosureMutated (Capability *cap, StgClosure *p)
 
 #if defined(THREADED_RTS)
 INLINE_HEADER rtsBool
-emptySparkPoolCap (Capability *cap) 
+emptySparkPoolCap (Capability *cap)
 { return looksEmpty(cap->sparks); }
 
 INLINE_HEADER nat
-sparkPoolSizeCap (Capability *cap) 
+sparkPoolSizeCap (Capability *cap)
 { return sparkPoolSize(cap->sparks); }
 
 INLINE_HEADER void
-discardSparksCap (Capability *cap) 
+discardSparksCap (Capability *cap)
 { return discardSparks(cap->sparks); }
 #endif
 
diff --git a/rts/RtsFlags.c b/rts/RtsFlags.c
index abeffc7..a866df5 100644
--- a/rts/RtsFlags.c
+++ b/rts/RtsFlags.c
@@ -44,7 +44,7 @@ wchar_t **win32_prog_argv = NULL;
 #endif
 
 /*
- * constants, used later 
+ * constants, used later
  */
 #define RTS 1
 #define PGM 0
@@ -271,7 +271,7 @@ usage_text[] = {
 "    -hb<bio>...  closures with specified biographies (lag,drag,void,use)",
 "",
 "  -R<size>       Set the maximum retainer set size (default: 8)",
-"", 
+"",
 "  -L<chars>      Maximum length of a cost-centre stack in a heap profile",
 "                 (default: 25)",
 "",
@@ -390,9 +390,9 @@ static void splitRtsFlags(char *s)
 	while (isspace(*c1)) { c1++; };
 	c2 = c1;
 	while (!isspace(*c2) && *c2 != '\0') { c2++; };
-	
+
 	if (c1 == c2) { break; }
-	
+
         s = stgMallocBytes(c2-c1+1, "RtsFlags.c:splitRtsFlags()");
         strncpy(s, c1, c2-c1);
         s[c2-c1] = '\0';
@@ -401,7 +401,7 @@ static void splitRtsFlags(char *s)
 	c1 = c2;
     } while (*c1 != '\0');
 }
-    
+
 /* -----------------------------------------------------------------------------
    Parse the command line arguments, collecting options for the RTS.
 
@@ -695,14 +695,14 @@ error = rtsTrue;
 
 	      case 'F':
 	        RtsFlags.GcFlags.oldGenFactor = atof(rts_argv[arg]+2);
-	      
+
 		if (RtsFlags.GcFlags.oldGenFactor < 0)
 		  bad_option( rts_argv[arg] );
 		break;
-	      
+
 	      case 'D':
               DEBUG_BUILD_ONLY(
-	      { 
+	      {
 		  char *c;
 
 		  for (c  = rts_argv[arg] + 2; *c != '\0'; c++) {
@@ -822,7 +822,7 @@ error = rtsTrue;
 		  break;
 #endif
 
-    	      case 'I':	/* idle GC delay */
+              case 'I':	/* idle GC delay */
 		if (rts_argv[arg][2] == '\0') {
 		  /* use default */
 		} else {
@@ -847,7 +847,7 @@ error = rtsTrue;
 		  goto stats;
 
 	    stats:
-		{ 
+		{
 		    int r;
                     r = openStatsFile(rts_argv[arg]+2, NULL,
                                       &RtsFlags.GcFlags.statsFile);
@@ -886,7 +886,7 @@ error = rtsTrue;
 	      case 'R':
 		  PROFILING_BUILD_ONLY(
 		      RtsFlags.ProfFlags.maxRetainerSetSize = atof(rts_argv[arg]+2);
-  	          ) break;
+                  ) break;
 	      case 'L':
 		  PROFILING_BUILD_ONLY(
 		      RtsFlags.ProfFlags.ccsLength = atof(rts_argv[arg]+2);
@@ -950,7 +950,7 @@ error = rtsTrue;
 				RtsFlags.ProfFlags.modSelector = left;
 				break;
 			    case 'D':
-			    case 'd': // closure descr select 
+			    case 'd': // closure descr select
 				RtsFlags.ProfFlags.descrSelector = left;
 				break;
 			    case 'Y':
@@ -1004,16 +1004,16 @@ error = rtsTrue;
 			  break;
 		    }
 		    break;
-		      
+
 		default:
 		    errorBelch("invalid heap profile option: %s",rts_argv[arg]);
 		    error = rtsTrue;
 		}
-		) 
+		)
 #endif /* PROFILING */
-    	    	break;
+                break;
 
-    	      case 'i':	/* heap sample interval */
+              case 'i':	/* heap sample interval */
 		if (rts_argv[arg][2] == '\0') {
 		  /* use default */
 		} else {
@@ -1026,9 +1026,9 @@ error = rtsTrue;
 		break;
 
 	      /* =========== CONCURRENT ========================= */
-    	      case 'C':	/* context switch interval */
+              case 'C':	/* context switch interval */
 		if (rts_argv[arg][2] == '\0')
-    	    	    RtsFlags.ConcFlags.ctxtSwitchTime = 0;
+                    RtsFlags.ConcFlags.ctxtSwitchTime = 0;
 		else {
 		    I_ cst; /* tmp */
 
@@ -1036,7 +1036,7 @@ error = rtsTrue;
 		    cst = (I_) ((atof(rts_argv[arg]+2) * 1000));
 		    RtsFlags.ConcFlags.ctxtSwitchTime = cst;
 		}
-    	    	break;
+                break;
 
               case 'V': /* master tick interval */
                 if (rts_argv[arg][2] == '\0') {
@@ -1152,7 +1152,7 @@ error = rtsTrue;
 
 		RtsFlags.TickyFlags.showTickyStats = rtsTrue;
 
-		{ 
+		{
 		    int r;
                     r = openStatsFile(rts_argv[arg]+2,
                                       TICKY_FILENAME_FMT,
@@ -1293,7 +1293,7 @@ static void normaliseRtsOpts (void)
 
     if (RtsFlags.ProfFlags.profileInterval > 0) {
         RtsFlags.ProfFlags.profileIntervalTicks =
-            RtsFlags.ProfFlags.profileInterval / 
+            RtsFlags.ProfFlags.profileInterval /
             RtsFlags.MiscFlags.tickInterval;
     } else {
         RtsFlags.ProfFlags.profileIntervalTicks = 0;
@@ -1405,7 +1405,7 @@ decodeSize(const char *flag, nat offset, StgWord64 min, StgWord64 max)
         m = atof(s);
         c = s[strlen(s)-1];
 
-        if (c == 'g' || c == 'G') 
+        if (c == 'g' || c == 'G')
             m *= 1024*1024*1024;
         else if (c == 'm' || c == 'M')
             m *= 1024*1024;
@@ -1649,7 +1649,7 @@ void
 setWin32ProgArgv(int argc, wchar_t *argv[])
 {
 	int i;
-    
+
 	freeWin32ProgArgv();
 
     win32_prog_argc = argc;
@@ -1657,7 +1657,7 @@ setWin32ProgArgv(int argc, wchar_t *argv[])
 		win32_prog_argv = NULL;
 		return;
 	}
-	
+
     win32_prog_argv = stgCallocBytes(argc + 1, sizeof (wchar_t *),
                                     "setWin32ProgArgv 1");
     for (i = 0; i < argc; i++) {
diff --git a/rts/Schedule.c b/rts/Schedule.c
index 94c5e5f..2a48bc6 100644
--- a/rts/Schedule.c
+++ b/rts/Schedule.c
@@ -89,7 +89,7 @@ volatile StgWord recent_activity = ACTIVITY_YES;
  */
 volatile StgWord sched_state = SCHED_RUNNING;
 
-/*  This is used in `TSO.h' and gcc 2.96 insists that this variable actually 
+/*  This is used in `TSO.h' and gcc 2.96 insists that this variable actually
  *  exists - earlier gccs apparently didn't.
  *  -= chak
  */
@@ -170,8 +170,8 @@ static void deleteThread_(Capability *cap, StgTSO *tso);
 
    GRAN version:
      In a GranSim setup this loop iterates over the global event queue.
-     This revolves around the global event queue, which determines what 
-     to do next. Therefore, it's more complicated than either the 
+     This revolves around the global event queue, which determines what
+     to do next. Therefore, it's more complicated than either the
      concurrent or the parallel (GUM) setup.
   This version has been entirely removed (JB 2008/08).
 
@@ -180,7 +180,7 @@ static void deleteThread_(Capability *cap, StgTSO *tso);
      It starts with nothing to do (thus CurrentTSO == END_TSO_QUEUE),
      and sends out a fish whenever it has nothing to do; in-between
      doing the actual reductions (shared code below) it processes the
-     incoming messages and deals with delayed operations 
+     incoming messages and deals with delayed operations
      (see PendingFetches).
      This is not the ugliest code you could imagine, but it's bloody close.
 
@@ -203,7 +203,7 @@ schedule (Capability *initialCapability, Task *task)
 #if defined(THREADED_RTS)
   rtsBool first = rtsTrue;
 #endif
-  
+
   cap = initialCapability;
 
   // Pre-condition: this task owns initialCapability.
@@ -223,13 +223,13 @@ schedule (Capability *initialCapability, Task *task)
     // going via suspendThread()/resumeThread (i.e. a 'safe' foreign
     // call).
     if (cap->in_haskell) {
-    	  errorBelch("schedule: re-entered unsafely.\n"
-    		     "   Perhaps a 'foreign import unsafe' should be 'safe'?");
-    	  stg_exit(EXIT_FAILURE);
+          errorBelch("schedule: re-entered unsafely.\n"
+                     "   Perhaps a 'foreign import unsafe' should be 'safe'?");
+          stg_exit(EXIT_FAILURE);
     }
 
     // The interruption / shutdown sequence.
-    // 
+    //
     // In order to cleanly shut down the runtime, we want to:
     //   * make sure that all main threads return to their callers
     //     with the state 'Interrupted'.
@@ -247,7 +247,7 @@ schedule (Capability *initialCapability, Task *task)
     //     done by scheduleDoGC() for convenience (because GC already
     //     needs to acquire all the capabilities).  We can't kill
     //     threads involved in foreign calls.
-    // 
+    //
     //   * somebody calls shutdownHaskell(), which calls exitScheduler()
     //
     //   * sched_state := SCHED_SHUTTING_DOWN
@@ -259,9 +259,9 @@ schedule (Capability *initialCapability, Task *task)
     //   * eventually all Capabilities will shut down, and the RTS can
     //     exit.
     //
-    //   * We might be left with threads blocked in foreign calls, 
+    //   * We might be left with threads blocked in foreign calls,
     //     we should really attempt to kill these somehow (TODO);
-    
+
     switch (sched_state) {
     case SCHED_RUNNING:
 	break;
@@ -306,16 +306,16 @@ schedule (Capability *initialCapability, Task *task)
 #endif
 
     // Normally, the only way we can get here with no threads to
-    // run is if a keyboard interrupt received during 
+    // run is if a keyboard interrupt received during
     // scheduleCheckBlockedThreads() or scheduleDetectDeadlock().
     // Additionally, it is not fatal for the
     // threaded RTS to reach here with no threads to run.
     //
     // win32: might be here due to awaitEvent() being abandoned
     // as a result of a console event having been delivered.
-    
+
 #if defined(THREADED_RTS)
-    if (first) 
+    if (first)
     {
     // XXX: ToDo
     //     // don't yield the first time, we want a chance to run this
@@ -336,7 +336,7 @@ schedule (Capability *initialCapability, Task *task)
     }
 #endif
 
-    // 
+    //
     // Get a thread to run
     //
     t = popRunQueue(cap);
@@ -350,7 +350,7 @@ schedule (Capability *initialCapability, Task *task)
     // If not, we have to pass our capability to the right task.
     {
         InCall *bound = t->bound;
-      
+
 	if (bound) {
 	    if (bound->task == task) {
 		// yes, the Haskell thread is bound to the current native thread
@@ -364,14 +364,14 @@ schedule (Capability *initialCapability, Task *task)
 	    }
 	} else {
 	    // The thread we want to run is unbound.
-	    if (task->incall->tso) { 
+	    if (task->incall->tso) {
 		debugTrace(DEBUG_sched,
 			   "this OS thread cannot run thread %lu",
                            (unsigned long)t->id);
 		// no, the current native thread is bound to a different
 		// Haskell thread, so pass it to any worker thread
 		pushOnRunQueue(cap,t);
-		continue; 
+		continue;
 	    }
 	}
     }
@@ -394,7 +394,7 @@ schedule (Capability *initialCapability, Task *task)
 	&& !emptyThreadQueues(cap)) {
 	cap->context_switch = 1;
     }
-	 
+
 run_thread:
 
     // CurrentTSO is the thread to run.  t might be different if we
@@ -405,7 +405,7 @@ run_thread:
     startHeapProfTimer();
 
     // ----------------------------------------------------------------------
-    // Run the current thread 
+    // Run the current thread
 
     ASSERT_FULL_CAPABILITY_INVARIANTS(cap,task);
     ASSERT(t->cap == cap);
@@ -444,13 +444,13 @@ run_thread:
     traceEventRunThread(cap, t);
 
     switch (prev_what_next) {
-	
+
     case ThreadKilled:
     case ThreadComplete:
 	/* Thread already finished, return to scheduler. */
 	ret = ThreadFinished;
 	break;
-	
+
     case ThreadRunGHC:
     {
 	StgRegTable *r;
@@ -459,12 +459,12 @@ run_thread:
 	ret = r->rRet;
 	break;
     }
-    
+
     case ThreadInterpret:
 	cap = interpretBCO(cap);
 	ret = cap->r.rRet;
 	break;
-	
+
     default:
 	barf("schedule: invalid what_next field");
     }
@@ -500,13 +500,13 @@ run_thread:
     ASSERT(t->cap == cap);
 
     // ----------------------------------------------------------------------
-    
+
     // Costs for the scheduler are assigned to CCS_SYSTEM
     stopHeapProfTimer();
 #if defined(PROFILING)
     CCCS = CCS_SYSTEM;
 #endif
-    
+
     schedulePostRunThread(cap,t);
 
     ready_to_gc = rtsFalse;
@@ -526,7 +526,7 @@ run_thread:
     case ThreadYielding:
 	if (scheduleHandleYield(cap, t, prev_what_next)) {
             // shortcut for switching between compiler/interpreter:
-	    goto run_thread; 
+	    goto run_thread;
 	}
 	break;
 
@@ -580,7 +580,7 @@ removeFromRunQueue (Capability *cap, StgTSO *tso)
 static void
 schedulePreLoop(void)
 {
-  // initialisation for scheduler - what cannot go into initScheduler()  
+  // initialisation for scheduler - what cannot go into initScheduler()
 }
 
 /* -----------------------------------------------------------------------------
@@ -613,7 +613,7 @@ shouldYieldCapability (Capability *cap, Task *task)
     //   - the thread at the head of the run queue cannot be run
     //     by this Task (it is bound to another Task, or it is unbound
     //     and this task it bound).
-    return (waiting_for_gc || 
+    return (waiting_for_gc ||
             cap->returning_tasks_hd != NULL ||
             (!emptyRunQueue(cap) && (task->incall->tso == NULL
                                      ? cap->run_queue_hd->bound != NULL
@@ -623,7 +623,7 @@ shouldYieldCapability (Capability *cap, Task *task)
 // This is the single place where a Task goes to sleep.  There are
 // two reasons it might need to sleep:
 //    - there are no threads to run
-//    - we need to yield this Capability to someone else 
+//    - we need to yield this Capability to someone else
 //      (see shouldYieldCapability())
 //
 // Careful: the scheduler loop is quite delicate.  Make sure you run
@@ -637,7 +637,7 @@ scheduleYield (Capability **pcap, Task *task)
 
     // if we have work, and we don't need to give up the Capability, continue.
     //
-    if (!shouldYieldCapability(cap,task) && 
+    if (!shouldYieldCapability(cap,task) &&
         (!emptyRunQueue(cap) ||
          !emptyInbox(cap) ||
          sched_state >= SCHED_INTERRUPTING))
@@ -646,7 +646,7 @@ scheduleYield (Capability **pcap, Task *task)
     // otherwise yield (sleep), and keep yielding if necessary.
     do {
         yieldCapability(&cap,task);
-    } 
+    }
     while (shouldYieldCapability(cap,task));
 
     // note there may still be no threads on the run queue at this
@@ -656,7 +656,7 @@ scheduleYield (Capability **pcap, Task *task)
     return;
 }
 #endif
-    
+
 /* -----------------------------------------------------------------------------
  * schedulePushWork()
  *
@@ -664,7 +664,7 @@ scheduleYield (Capability **pcap, Task *task)
  * -------------------------------------------------------------------------- */
 
 static void
-schedulePushWork(Capability *cap USED_IF_THREADS, 
+schedulePushWork(Capability *cap USED_IF_THREADS,
 		 Task *task      USED_IF_THREADS)
 {
   /* following code not for PARALLEL_HASKELL. I kept the call general,
@@ -693,7 +693,7 @@ schedulePushWork(Capability *cap USED_IF_THREADS,
 	    if (!emptyRunQueue(cap0)
                 || cap->returning_tasks_hd != NULL
                 || cap->inbox != (Message*)END_TSO_QUEUE) {
-		// it already has some work, we just grabbed it at 
+		// it already has some work, we just grabbed it at
 		// the wrong moment.  Or maybe it's deadlocked!
 		releaseCapability(cap0);
 	    } else {
@@ -707,7 +707,7 @@ schedulePushWork(Capability *cap USED_IF_THREADS,
     // probably the simplest thing we could do; improvements we might
     // want to do include:
     //
-    //   - giving high priority to moving relatively new threads, on 
+    //   - giving high priority to moving relatively new threads, on
     //     the gournds that they haven't had time to build up a
     //     working set in the cache on this CPU/Capability.
     //
@@ -719,9 +719,9 @@ schedulePushWork(Capability *cap USED_IF_THREADS,
 	rtsBool pushed_to_all;
 #endif
 
-	debugTrace(DEBUG_sched, 
-		   "cap %d: %s and %d free capabilities, sharing...", 
-		   cap->no, 
+	debugTrace(DEBUG_sched,
+		   "cap %d: %s and %d free capabilities, sharing...",
+		   cap->no,
 		   (!emptyRunQueue(cap) && cap->run_queue_hd->_link != END_TSO_QUEUE)?
 		   "excess threads on run queue":"sparks to share (>=2)",
 		   n_free_caps);
@@ -853,7 +853,7 @@ scheduleCheckBlockedThreads(Capability *cap USED_IF_NOT_THREADS)
 static void
 scheduleDetectDeadlock (Capability *cap, Task *task)
 {
-    /* 
+    /*
      * Detect deadlock: when we have no threads to run, there are no
      * threads blocked, waiting for I/O, or sleeping, and all the
      * other tasks are waiting for work, we must have a deadlock of
@@ -862,7 +862,7 @@ scheduleDetectDeadlock (Capability *cap, Task *task)
     if ( emptyThreadQueues(cap) )
     {
 #if defined(THREADED_RTS)
-	/* 
+	/*
 	 * In the threaded RTS, we only check for deadlock if there
 	 * has been no activity in a complete timeslice.  This means
 	 * we won't eagerly start a full GC just because we don't have
@@ -917,7 +917,7 @@ scheduleDetectDeadlock (Capability *cap, Task *task)
 	    case BlockedOnBlackHole:
 	    case BlockedOnMsgThrowTo:
 	    case BlockedOnMVar:
-		throwToSingleThreaded(cap, task->incall->tso, 
+		throwToSingleThreaded(cap, task->incall->tso,
 				      (StgClosure *)nonTermination_closure);
 		return;
 	    default:
@@ -944,11 +944,11 @@ scheduleSendPendingMessages(void)
         processFetches();
     }
 # endif
-    
+
     if (RtsFlags.ParFlags.BufferTime) {
 	// if we use message buffering, we must send away all message
 	// packets which have become too old...
-	sendOldBuffers(); 
+	sendOldBuffers();
     }
 }
 #endif
@@ -1022,7 +1022,7 @@ schedulePostRunThread (Capability *cap, StgTSO *t)
 {
     // We have to be able to catch transactions that are in an
     // infinite loop as a result of seeing an inconsistent view of
-    // memory, e.g. 
+    // memory, e.g.
     //
     //   atomically $ do
     //       [a,b] <- mapM readTVar [ta,tb]
@@ -1034,13 +1034,13 @@ schedulePostRunThread (Capability *cap, StgTSO *t)
         if (!stmValidateNestOfTransactions (t -> trec)) {
             debugTrace(DEBUG_sched | DEBUG_stm,
                        "trec %p found wasting its time", t);
-            
+
             // strip the stack back to the
             // ATOMICALLY_FRAME, aborting the (nested)
             // transaction, and saving the stack of any
             // partially-evaluated thunks on the heap.
             throwToSingleThreaded_(cap, t, NULL, rtsTrue);
-            
+
 //            ASSERT(get_itbl((StgClosure *)t->sp)->type == ATOMICALLY_FRAME);
         }
     }
@@ -1060,25 +1060,25 @@ scheduleHandleHeapOverflow( Capability *cap, StgTSO *t )
 	// if so, get one and push it on the front of the nursery.
 	bdescr *bd;
 	lnat blocks;
-	
+
 	blocks = (lnat)BLOCK_ROUND_UP(cap->r.rHpAlloc) / BLOCK_SIZE;
-	
+
         if (blocks > BLOCKS_PER_MBLOCK) {
             barf("allocation of %ld bytes too large (GHC should have complained at compile-time)", (long)cap->r.rHpAlloc);
         }
 
 	debugTrace(DEBUG_sched,
-		   "--<< thread %ld (%s) stopped: requesting a large block (size %ld)\n", 
+		   "--<< thread %ld (%s) stopped: requesting a large block (size %ld)\n",
 		   (long)t->id, what_next_strs[t->what_next], blocks);
-    
+
 	// don't do this if the nursery is (nearly) full, we'll GC first.
 	if (cap->r.rCurrentNursery->link != NULL ||
 	    cap->r.rNursery->n_blocks == 1) {  // paranoia to prevent infinite loop
 	                                       // if the nursery has only one block.
-	    
+
             bd = allocGroup_lock(blocks);
             cap->r.rNursery->n_blocks += blocks;
-	    
+
 	    // link the new group into the list
 	    bd->link = cap->r.rCurrentNursery;
 	    bd->u.back = cap->r.rCurrentNursery->u.back;
@@ -1086,9 +1086,9 @@ scheduleHandleHeapOverflow( Capability *cap, StgTSO *t )
 		cap->r.rCurrentNursery->u.back->link = bd;
 	    } else {
 		cap->r.rNursery->blocks = bd;
-	    }		  
+	    }
 	    cap->r.rCurrentNursery->u.back = bd;
-	    
+
 	    // initialise it as a nursery block.  We initialise the
 	    // step, gen_no, and flags field of *every* sub-block in
 	    // this large block, because this is easier than making
@@ -1096,7 +1096,7 @@ scheduleHandleHeapOverflow( Capability *cap, StgTSO *t )
 	    // block whenever we call Bdescr() (eg. evacuate() and
 	    // isAlive() in the GC would both have to do this, at
 	    // least).
-	    { 
+	    {
 		bdescr *x;
 		for (x = bd; x < bd + blocks; x++) {
                     initBdescr(x,g0,g0);
@@ -1104,14 +1104,14 @@ scheduleHandleHeapOverflow( Capability *cap, StgTSO *t )
 		    x->flags = 0;
 		}
 	    }
-	    
+
 	    // This assert can be a killer if the app is doing lots
 	    // of large block allocations.
 	    IF_DEBUG(sanity, checkNurserySanity(cap->r.rNursery));
-	    
+
 	    // now update the nursery to point to the new block
 	    cap->r.rCurrentNursery = bd;
-	    
+
 	    // we might be unlucky and have another thread get on the
 	    // run queue before us and steal the large block, but in that
 	    // case the thread will just end up requesting another large
@@ -1120,7 +1120,7 @@ scheduleHandleHeapOverflow( Capability *cap, StgTSO *t )
 	    return rtsFalse;  /* not actually GC'ing */
 	}
     }
-    
+
     if (cap->r.rHpLim == NULL || cap->context_switch) {
         // Sometimes we miss a context switch, e.g. when calling
         // primitives in a tight loop, MAYBE_GC() doesn't check the
@@ -1149,13 +1149,13 @@ scheduleHandleYield( Capability *cap, StgTSO *t, nat prev_what_next )
      */
 
     ASSERT(t->_link == END_TSO_QUEUE);
-    
+
     // Shortcut if we're just switching evaluators: don't bother
     // doing stack squeezing (which can be expensive), just run the
     // thread.
     if (cap->context_switch == 0 && t->what_next != prev_what_next) {
 	debugTrace(DEBUG_sched,
-		   "--<< thread %ld (%s) stopped to switch evaluators", 
+		   "--<< thread %ld (%s) stopped to switch evaluators",
 		   (long)t->id, what_next_strs[t->what_next]);
 	return rtsTrue;
     }
@@ -1167,7 +1167,7 @@ scheduleHandleYield( Capability *cap, StgTSO *t, nat prev_what_next )
     // penalises threads that do a lot of allocation, but that seems
     // better than the alternative.
     cap->context_switch = 0;
-    
+
     IF_DEBUG(sanity,
 	     //debugBelch("&& Doing sanity check on yielding TSO %ld.", t->id);
 	     checkTSO(t));
@@ -1224,7 +1224,7 @@ scheduleHandleThreadFinished (Capability *cap STG_UNUSED, Task *task, StgTSO *t)
 
       //
       // Check whether the thread that just completed was a bound
-      // thread, and if so return with the result.  
+      // thread, and if so return with the result.
       //
       // There is an assumption here that all thread completion goes
       // through this point; we need to make sure that if a thread
@@ -1320,7 +1320,7 @@ scheduleDoGC (Capability *cap, Task *task USED_IF_THREADS, rtsBool force_major)
 {
     rtsBool heap_census;
 #ifdef THREADED_RTS
-    /* extern static volatile StgWord waiting_for_gc; 
+    /* extern static volatile StgWord waiting_for_gc;
        lives inside capability.c */
     rtsBool gc_type, prev_pending_gc;
     nat i;
@@ -1346,7 +1346,7 @@ scheduleDoGC (Capability *cap, Task *task USED_IF_THREADS, rtsBool force_major)
 
     // In order to GC, there must be no threads running Haskell code.
     // Therefore, the GC thread needs to hold *all* the capabilities,
-    // and release them after the GC has completed.  
+    // and release them after the GC has completed.
     //
     // This seems to be the simplest way: previous attempts involved
     // making all the threads with capabilities give up their
@@ -1362,7 +1362,7 @@ scheduleDoGC (Capability *cap, Task *task USED_IF_THREADS, rtsBool force_major)
     prev_pending_gc = cas(&waiting_for_gc, 0, gc_type);
     if (prev_pending_gc) {
 	do {
-	    debugTrace(DEBUG_sched, "someone else is trying to GC (%d)...", 
+	    debugTrace(DEBUG_sched, "someone else is trying to GC (%d)...",
                        prev_pending_gc);
             ASSERT(cap);
             yieldCapability(&cap,task);
@@ -1374,7 +1374,7 @@ scheduleDoGC (Capability *cap, Task *task USED_IF_THREADS, rtsBool force_major)
 
     // The final shutdown GC is always single-threaded, because it's
     // possible that some of the Capabilities have no worker threads.
-    
+
     if (gc_type == PENDING_GC_SEQ)
     {
         traceEventRequestSeqGc(cap);
@@ -1410,7 +1410,7 @@ scheduleDoGC (Capability *cap, Task *task USED_IF_THREADS, rtsBool force_major)
         // multi-threaded GC: make sure all the Capabilities donate one
         // GC thread each.
         waitForGcThreads(cap);
-        
+
 #if defined(THREADED_RTS)
         // Stable point where we can do a global check on our spark counters
         ASSERT(checkSparkCountInvariant());
@@ -1431,7 +1431,7 @@ delete_threads_and_gc:
 	deleteAllThreads(cap);
 	sched_state = SCHED_SHUTTING_DOWN;
     }
-    
+
     heap_census = scheduleNeedHeapProfile(rtsTrue);
 
     traceEventGcStart(cap);
@@ -1500,7 +1500,7 @@ delete_threads_and_gc:
     }
 
 #ifdef SPARKBALANCE
-    /* JB 
+    /* JB
        Once we are all together... this would be the place to balance all
        spark pools. No concurrent stealing or adding of new sparks can
        occur. Should be defined in Sparks.c. */
@@ -1543,7 +1543,7 @@ forkProcess(HsStablePtr *entry
     StgTSO* t,*next;
     Capability *cap;
     nat g;
-    
+
 #if defined(THREADED_RTS)
     if (RtsFlags.ParFlags.nNodes > 1) {
 	errorBelch("forking not supported with +RTS -N<n> greater than 1");
@@ -1552,10 +1552,10 @@ forkProcess(HsStablePtr *entry
 #endif
 
     debugTrace(DEBUG_sched, "forking!");
-    
+
     // ToDo: for SMP, we should probably acquire *all* the capabilities
     cap = rts_lock();
-    
+
     // no funny business: hold locks while we fork, otherwise if some
     // other thread is holding a lock when the fork happens, the data
     // structure protected by the lock will forever be in an
@@ -1571,9 +1571,9 @@ forkProcess(HsStablePtr *entry
 #endif
 
     pid = fork();
-    
+
     if (pid) { // parent
-	
+
         startTimer(); // #4074
 
         RELEASE_LOCK(&sched_mutex);
@@ -1583,9 +1583,9 @@ forkProcess(HsStablePtr *entry
 	// just return the pid
 	rts_unlock(cap);
 	return pid;
-	
+
     } else { // child
-	
+
 #if defined(THREADED_RTS)
         initMutex(&sched_mutex);
         initMutex(&cap->lock);
@@ -1618,7 +1618,7 @@ forkProcess(HsStablePtr *entry
                 t->bound = NULL;
           }
 	}
-	
+
 	// Empty the run queue.  It seems tempting to let all the
 	// killed threads stay on the run queue as zombies to be
 	// cleaned up later, but some of them correspond to bound
@@ -1658,7 +1658,7 @@ forkProcess(HsStablePtr *entry
 
 	cap = rts_evalStableIO(cap, entry, NULL);  // run the action
 	rts_checkSchedStatus("forkProcess",cap);
-	
+
 	rts_unlock(cap);
 	hs_exit();                      // clean up and exit
 	stg_exit(EXIT_SUCCESS);
@@ -1671,7 +1671,7 @@ forkProcess(HsStablePtr *entry
 /* ---------------------------------------------------------------------------
  * Delete all the threads in the system
  * ------------------------------------------------------------------------- */
-   
+
 static void
 deleteAllThreads ( Capability *cap )
 {
@@ -1709,7 +1709,7 @@ STATIC_INLINE void
 suspendTask (Capability *cap, Task *task)
 {
     InCall *incall;
-    
+
     incall = task->incall;
     ASSERT(incall->next == NULL && incall->prev == NULL);
     incall->next = cap->suspended_ccalls;
@@ -1740,7 +1740,7 @@ recoverSuspendedTask (Capability *cap, Task *task)
 
 /* ---------------------------------------------------------------------------
  * Suspending & resuming Haskell threads.
- * 
+ *
  * When making a "safe" call to C (aka _ccall_GC), the task gives back
  * its capability before calling the C function.  This allows another
  * task to pick up the capability and carry on running Haskell
@@ -1756,7 +1756,7 @@ recoverSuspendedTask (Capability *cap, Task *task)
  * unceremoniously terminated and should be scheduled on an
  * unbound worker thread.
  * ------------------------------------------------------------------------- */
-   
+
 void *
 suspendThread (StgRegTable *reg, rtsBool interruptible)
 {
@@ -1802,7 +1802,7 @@ suspendThread (StgRegTable *reg, rtsBool interruptible)
   suspendTask(cap,task);
   cap->in_haskell = rtsFalse;
   releaseCapability_(cap,rtsFalse);
-  
+
   RELEASE_LOCK(&cap->lock);
 
   errno = saved_errno;
@@ -1848,7 +1848,7 @@ resumeThread (void *task_)
     tso->_link = END_TSO_QUEUE; // no write barrier reqd
 
     traceEventRunThread(cap, tso);
-    
+
     /* Reset blocking status */
     tso->why_blocked  = NotBlocked;
 
@@ -1858,7 +1858,7 @@ resumeThread (void *task_)
             maybePerformBlockedException(cap,tso);
         }
     }
-    
+
     cap->r.rCurrentTSO = tso;
     cap->in_haskell = rtsTrue;
     errno = saved_errno;
@@ -1980,7 +1980,7 @@ void scheduleWorker (Capability *cap, Task *task)
  *
  * ------------------------------------------------------------------------ */
 
-void 
+void
 initScheduler(void)
 {
 #if !defined(THREADED_RTS)
@@ -1997,7 +1997,7 @@ initScheduler(void)
    * the scheduler. */
   initMutex(&sched_mutex);
 #endif
-  
+
   ACQUIRE_LOCK(&sched_mutex);
 
   /* A capability holds the state a native thread needs in
@@ -2017,7 +2017,7 @@ initScheduler(void)
    * bound thread on Capability 0 pretty soon, so we don't want a
    * worker task hogging it.
    */
-  { 
+  {
       nat i;
       Capability *cap;
       for (i = 1; i < n_capabilities; i++) {
@@ -2078,14 +2078,14 @@ freeScheduler( void )
 #endif
 }
 
-void markScheduler (evac_fn evac USED_IF_NOT_THREADS, 
+void markScheduler (evac_fn evac USED_IF_NOT_THREADS,
                     void *user USED_IF_NOT_THREADS)
 {
 #if !defined(THREADED_RTS)
     evac(user, (StgClosure **)(void *)&blocked_queue_hd);
     evac(user, (StgClosure **)(void *)&blocked_queue_tl);
     evac(user, (StgClosure **)(void *)&sleeping_queue);
-#endif 
+#endif
 }
 
 /* -----------------------------------------------------------------------------
@@ -2102,7 +2102,7 @@ performGC_(rtsBool force_major)
     Task *task;
 
     // We must grab a new Task here, because the existing Task may be
-    // associated with a particular Capability, and chained onto the 
+    // associated with a particular Capability, and chained onto the
     // suspended_ccalls queue.
     task = newBoundTask();
 
@@ -2126,7 +2126,7 @@ performMajorGC(void)
 
 /* ---------------------------------------------------------------------------
    Interrupt execution
-   - usually called inside a signal handler so it mustn't do anything fancy.   
+   - usually called inside a signal handler so it mustn't do anything fancy.
    ------------------------------------------------------------------------ */
 
 void
@@ -2141,7 +2141,7 @@ interruptStgRts(void)
 
 /* -----------------------------------------------------------------------------
    Wake up the RTS
-   
+
    This function causes at least one OS thread to wake up and run the
    scheduler loop.  It is invoked when the RTS might be deadlocked, or
    an external event has arrived that may need servicing (eg. a
@@ -2175,7 +2175,7 @@ deleteThread (Capability *cap STG_UNUSED, StgTSO *tso)
 {
     // NOTE: must only be called on a TSO that we have exclusive
     // access to, because we will call throwToSingleThreaded() below.
-    // The TSO must be on the run queue of the Capability we own, or 
+    // The TSO must be on the run queue of the Capability we own, or
     // we must own all Capabilities.
 
     if (tso->why_blocked != BlockedOnCCall &&
@@ -2202,7 +2202,7 @@ deleteThread_(Capability *cap, StgTSO *tso)
 
 /* -----------------------------------------------------------------------------
    raiseExceptionHelper
-   
+
    This function is called by the raise# primitve, just so that we can
    move some of the tricky bits of raising an exception from C-- into
    C.  Who knows, it might be a useful re-useable thing here too.
@@ -2231,7 +2231,7 @@ raiseExceptionHelper (StgRegTable *reg, StgTSO *tso, StgClosure *exception)
     // use MIN_UPD_SIZE.
     //
     // raise_closure = (StgClosure *)RET_STGCALL1(P_,allocate,
-    // 				       sizeofW(StgClosure)+1);
+    //                                 sizeofW(StgClosure)+1);
     //
 
     //
@@ -2244,11 +2244,11 @@ raiseExceptionHelper (StgRegTable *reg, StgTSO *tso, StgClosure *exception)
 	info = get_ret_itbl((StgClosure *)p);
 	next = p + stack_frame_sizeW((StgClosure *)p);
 	switch (info->i.type) {
-	    
+
 	case UPDATE_FRAME:
 	    // Only create raise_closure if we need to.
 	    if (raise_closure == NULL) {
-		raise_closure = 
+		raise_closure =
 		    (StgThunk *)allocate(cap,sizeofW(StgThunk)+1);
 		SET_HDR(raise_closure, &stg_raise_info, CCCS);
 		raise_closure->payload[0] = exception;
@@ -2262,7 +2262,7 @@ raiseExceptionHelper (StgRegTable *reg, StgTSO *tso, StgClosure *exception)
 	    debugTrace(DEBUG_stm, "found ATOMICALLY_FRAME at %p", p);
             tso->stackobj->sp = p;
             return ATOMICALLY_FRAME;
-	    
+
 	case CATCH_FRAME:
             tso->stackobj->sp = p;
 	    return CATCH_FRAME;
@@ -2271,7 +2271,7 @@ raiseExceptionHelper (StgRegTable *reg, StgTSO *tso, StgClosure *exception)
 	    debugTrace(DEBUG_stm, "found CATCH_STM_FRAME at %p", p);
             tso->stackobj->sp = p;
             return CATCH_STM_FRAME;
-	    
+
         case UNDERFLOW_FRAME:
             tso->stackobj->sp = p;
             threadStackUnderflow(cap,tso);
@@ -2284,7 +2284,7 @@ raiseExceptionHelper (StgRegTable *reg, StgTSO *tso, StgClosure *exception)
 
         case CATCH_RETRY_FRAME:
 	default:
-	    p = next; 
+	    p = next;
 	    continue;
 	}
     }
@@ -2295,10 +2295,10 @@ raiseExceptionHelper (StgRegTable *reg, StgTSO *tso, StgClosure *exception)
    findRetryFrameHelper
 
    This function is called by the retry# primitive.  It traverses the stack
-   leaving tso->sp referring to the frame which should handle the retry.  
+   leaving tso->sp referring to the frame which should handle the retry.
 
-   This should either be a CATCH_RETRY_FRAME (if the retry# is within an orElse#) 
-   or should be a ATOMICALLY_FRAME (if the retry# reaches the top level).  
+   This should either be a CATCH_RETRY_FRAME (if the retry# is within an orElse#)
+   or should be a ATOMICALLY_FRAME (if the retry# reaches the top level).
 
    We skip CATCH_STM_FRAMEs (aborting and rolling back the nested tx that they
    create) because retries are not considered to be exceptions, despite the
@@ -2319,19 +2319,19 @@ findRetryFrameHelper (Capability *cap, StgTSO *tso)
     info = get_ret_itbl((StgClosure *)p);
     next = p + stack_frame_sizeW((StgClosure *)p);
     switch (info->i.type) {
-      
+
     case ATOMICALLY_FRAME:
 	debugTrace(DEBUG_stm,
 		   "found ATOMICALLY_FRAME at %p during retry", p);
         tso->stackobj->sp = p;
 	return ATOMICALLY_FRAME;
-      
+
     case CATCH_RETRY_FRAME:
 	debugTrace(DEBUG_stm,
 		   "found CATCH_RETRY_FRAME at %p during retrry", p);
         tso->stackobj->sp = p;
 	return CATCH_RETRY_FRAME;
-      
+
     case CATCH_STM_FRAME: {
         StgTRecHeader *trec = tso -> trec;
 	StgTRecHeader *outer = trec -> enclosing_trec;
@@ -2341,10 +2341,10 @@ findRetryFrameHelper (Capability *cap, StgTSO *tso)
         stmAbortTransaction(cap, trec);
         stmFreeAbortedTRec(cap, trec);
 	tso -> trec = outer;
-        p = next; 
+        p = next;
         continue;
     }
-      
+
     case UNDERFLOW_FRAME:
         threadStackUnderflow(cap,tso);
         p = tso->stackobj->sp;
@@ -2353,7 +2353,7 @@ findRetryFrameHelper (Capability *cap, StgTSO *tso)
     default:
       ASSERT(info->i.type != CATCH_FRAME);
       ASSERT(info->i.type != STOP_FRAME);
-      p = next; 
+      p = next;
       continue;
     }
   }
@@ -2384,10 +2384,10 @@ resurrectThreads (StgTSO *threads)
 	gen->threads = tso;
 
 	debugTrace(DEBUG_sched, "resurrecting thread %lu", (unsigned long)tso->id);
-	
+
 	// Wake up the thread on the Capability it was last on
 	cap = tso->cap;
-	
+
 	switch (tso->why_blocked) {
 	case BlockedOnMVar:
 	    /* Called by GC - sched_mutex lock is currently held. */
diff --git a/rts/Schedule.h b/rts/Schedule.h
index 549f555..b1141cd 100644
--- a/rts/Schedule.h
+++ b/rts/Schedule.h
@@ -2,7 +2,7 @@
  *
  * (c) The GHC Team 1998-2005
  *
- * Prototypes for functions in Schedule.c 
+ * Prototypes for functions in Schedule.c
  * (RTS internal scheduler interface)
  *
  * -------------------------------------------------------------------------*/
@@ -34,7 +34,7 @@ void scheduleThread (Capability *cap, StgTSO *tso);
 void scheduleThreadOn(Capability *cap, StgWord cpu, StgTSO *tso);
 
 /* wakeUpRts()
- * 
+ *
  * Causes an OS thread to wake up and run the scheduler, if necessary.
  */
 #if defined(THREADED_RTS)
@@ -60,7 +60,7 @@ void scheduleWorker (Capability *cap, Task *task);
 
 extern volatile StgWord sched_state;
 
-/* 
+/*
  * flag that tracks whether we have done any execution in this time slice.
  */
 #define ACTIVITY_YES      0 /* there has been activity in the current slice */
@@ -152,7 +152,7 @@ pushOnRunQueue (Capability *cap, StgTSO *tso)
  */
 INLINE_HEADER StgTSO *
 popRunQueue (Capability *cap)
-{ 
+{
     StgTSO *t = cap->run_queue_hd;
     ASSERT(t != END_TSO_QUEUE);
     cap->run_queue_hd = t->_link;
@@ -218,4 +218,3 @@ emptyThreadQueues(Capability *cap)
 #include "EndPrivate.h"
 
 #endif /* SCHEDULE_H */
-
diff --git a/rts/Trace.c b/rts/Trace.c
index 7d856d6..f7cba84 100644
--- a/rts/Trace.c
+++ b/rts/Trace.c
@@ -168,8 +168,8 @@ static char *thread_stop_reasons[] = {
 #endif
 
 #ifdef DEBUG
-static void traceSchedEvent_stderr (Capability *cap, EventTypeNum tag, 
-                                    StgTSO *tso, 
+static void traceSchedEvent_stderr (Capability *cap, EventTypeNum tag,
+                                    StgTSO *tso,
                                     StgWord info1 STG_UNUSED,
                                     StgWord info2 STG_UNUSED)
 {
@@ -178,38 +178,38 @@ static void traceSchedEvent_stderr (Capability *cap, EventTypeNum tag,
     tracePreface();
     switch (tag) {
     case EVENT_CREATE_THREAD:   // (cap, thread)
-        debugBelch("cap %d: created thread %lu\n", 
+        debugBelch("cap %d: created thread %lu\n",
                    cap->no, (lnat)tso->id);
         break;
     case EVENT_RUN_THREAD:      //  (cap, thread)
-        debugBelch("cap %d: running thread %lu (%s)\n", 
+        debugBelch("cap %d: running thread %lu (%s)\n",
                    cap->no, (lnat)tso->id, what_next_strs[tso->what_next]);
         break;
     case EVENT_THREAD_RUNNABLE: // (cap, thread)
-        debugBelch("cap %d: thread %lu appended to run queue\n", 
+        debugBelch("cap %d: thread %lu appended to run queue\n",
                    cap->no, (lnat)tso->id);
         break;
     case EVENT_RUN_SPARK:       // (cap, thread)
-        debugBelch("cap %d: thread %lu running a spark\n", 
+        debugBelch("cap %d: thread %lu running a spark\n",
                    cap->no, (lnat)tso->id);
         break;
     case EVENT_CREATE_SPARK_THREAD: // (cap, spark_thread)
-        debugBelch("cap %d: creating spark thread %lu\n", 
+        debugBelch("cap %d: creating spark thread %lu\n",
                    cap->no, (long)info1);
         break;
     case EVENT_MIGRATE_THREAD:  // (cap, thread, new_cap)
-        debugBelch("cap %d: thread %lu migrating to cap %d\n", 
+        debugBelch("cap %d: thread %lu migrating to cap %d\n",
                    cap->no, (lnat)tso->id, (int)info1);
         break;
     case EVENT_STEAL_SPARK:     // (cap, thread, victim_cap)
-        debugBelch("cap %d: thread %lu stealing a spark from cap %d\n", 
+        debugBelch("cap %d: thread %lu stealing a spark from cap %d\n",
                    cap->no, (lnat)tso->id, (int)info1);
         break;
     case EVENT_THREAD_WAKEUP:   // (cap, thread, info1_cap)
-        debugBelch("cap %d: waking up thread %lu on cap %d\n", 
+        debugBelch("cap %d: waking up thread %lu on cap %d\n",
                    cap->no, (lnat)tso->id, (int)info1);
         break;
-        
+
     case EVENT_STOP_THREAD:     // (cap, thread, status)
         if (info1 == 6 + BlockedOnBlackHole) {
             debugBelch("cap %d: thread %lu stopped (blocked on black hole owned by thread %lu)\n",
@@ -244,7 +244,7 @@ static void traceSchedEvent_stderr (Capability *cap, EventTypeNum tag,
         debugBelch("cap %d: GC done\n", cap->no);
         break;
     default:
-        debugBelch("cap %d: thread %lu: event %d\n\n", 
+        debugBelch("cap %d: thread %lu: event %d\n\n",
                    cap->no, (lnat)tso->id, tag);
         break;
     }
@@ -253,7 +253,7 @@ static void traceSchedEvent_stderr (Capability *cap, EventTypeNum tag,
 }
 #endif
 
-void traceSchedEvent_ (Capability *cap, EventTypeNum tag, 
+void traceSchedEvent_ (Capability *cap, EventTypeNum tag,
                        StgTSO *tso, StgWord info1, StgWord info2)
 {
 #ifdef DEBUG
@@ -391,7 +391,7 @@ void traceCap_(Capability *cap, char *msg, ...)
 {
     va_list ap;
     va_start(ap,msg);
-    
+
 #ifdef DEBUG
     if (RtsFlags.TraceFlags.tracing == TRACE_STDERR) {
         traceCap_stderr(cap, msg, ap);
diff --git a/rts/Trace.h b/rts/Trace.h
index b29df52..285caf6 100644
--- a/rts/Trace.h
+++ b/rts/Trace.h
@@ -71,7 +71,7 @@ extern int TRACE_spark;
 // the not-taken case to be as efficient as possible, a simple
 // test-and-jump, and with inline functions gcc seemed to move some of
 // the instructions from the branch up before the test.
-// 
+//
 // -----------------------------------------------------------------------------
 
 #ifdef DEBUG
@@ -81,7 +81,7 @@ void traceEnd (void);
 
 #ifdef TRACING
 
-/* 
+/*
  * Record a scheduler event
  */
 #define traceSchedEvent(cap, tag, tso, other)   \
@@ -99,7 +99,7 @@ void traceEnd (void);
         traceSchedEvent_(cap, tag, tso, other, 0); \
     }
 
-void traceSchedEvent_ (Capability *cap, EventTypeNum tag, 
+void traceSchedEvent_ (Capability *cap, EventTypeNum tag,
                        StgTSO *tso, StgWord info1, StgWord info2);
 
 
@@ -117,7 +117,7 @@ void traceEvent_ (Capability *cap, EventTypeNum tag);
 // ##__VA_ARGS syntax is a gcc extension, which allows the variable
 // argument list to be empty (see gcc docs for details).
 
-/* 
+/*
  * Emit a trace message on a particular Capability
  */
 #define traceCap(class, cap, msg, ...)          \
@@ -127,7 +127,7 @@ void traceEvent_ (Capability *cap, EventTypeNum tag);
 
 void traceCap_(Capability *cap, char *msg, ...);
 
-/* 
+/*
  * Emit a trace message
  */
 #define trace(class, msg, ...)                  \
@@ -137,12 +137,12 @@ void traceCap_(Capability *cap, char *msg, ...);
 
 void trace_(char *msg, ...);
 
-/* 
+/*
  * A message or event emitted by the program
  */
 void traceUserMsg(Capability *cap, char *msg);
 
-/* 
+/*
  * Emit a debug message (only when DEBUG is defined)
  */
 #ifdef DEBUG
@@ -163,7 +163,7 @@ void traceUserMsg(Capability *cap, char *msg);
 #define debugTraceCap(class, cap, str, ...) /* nothing */
 #endif
 
-/* 
+/*
  * Emit a message/event describing the state of a thread
  */
 #define traceThreadStatus(class, tso)           \
@@ -320,25 +320,25 @@ INLINE_HEADER void dtraceStartup (int num_caps STG_UNUSED) {};
 //   a file, even in the presence of dtrace.  This is, eg, useful when tracing
 //   on a server, but browsing trace information with ThreadScope on a local
 //   client.
-// 
+//
 // -----------------------------------------------------------------------------
 
-INLINE_HEADER void traceEventCreateThread(Capability *cap STG_UNUSED, 
+INLINE_HEADER void traceEventCreateThread(Capability *cap STG_UNUSED,
                                           StgTSO     *tso STG_UNUSED)
 {
     traceSchedEvent(cap, EVENT_CREATE_THREAD, tso, tso->stackobj->stack_size);
     dtraceCreateThread((EventCapNo)cap->no, (EventThreadID)tso->id);
 }
 
-INLINE_HEADER void traceEventRunThread(Capability *cap STG_UNUSED, 
+INLINE_HEADER void traceEventRunThread(Capability *cap STG_UNUSED,
                                        StgTSO     *tso STG_UNUSED)
 {
     traceSchedEvent(cap, EVENT_RUN_THREAD, tso, tso->what_next);
     dtraceRunThread((EventCapNo)cap->no, (EventThreadID)tso->id);
 }
 
-INLINE_HEADER void traceEventStopThread(Capability          *cap    STG_UNUSED, 
-                                        StgTSO              *tso    STG_UNUSED, 
+INLINE_HEADER void traceEventStopThread(Capability          *cap    STG_UNUSED,
+                                        StgTSO              *tso    STG_UNUSED,
                                         StgThreadReturnCode  status STG_UNUSED,
                                         StgWord32           info    STG_UNUSED)
 {
@@ -348,17 +348,17 @@ INLINE_HEADER void traceEventStopThread(Capability          *cap    STG_UNUSED,
 }
 
 // needs to be EXTERN_INLINE as it is used in another EXTERN_INLINE function
-EXTERN_INLINE void traceEventThreadRunnable(Capability *cap STG_UNUSED, 
+EXTERN_INLINE void traceEventThreadRunnable(Capability *cap STG_UNUSED,
                                             StgTSO     *tso STG_UNUSED);
 
-EXTERN_INLINE void traceEventThreadRunnable(Capability *cap STG_UNUSED, 
+EXTERN_INLINE void traceEventThreadRunnable(Capability *cap STG_UNUSED,
                                             StgTSO     *tso STG_UNUSED)
 {
     traceSchedEvent(cap, EVENT_THREAD_RUNNABLE, tso, 0);
     dtraceThreadRunnable((EventCapNo)cap->no, (EventThreadID)tso->id);
 }
 
-INLINE_HEADER void traceEventMigrateThread(Capability *cap     STG_UNUSED, 
+INLINE_HEADER void traceEventMigrateThread(Capability *cap     STG_UNUSED,
                                            StgTSO     *tso     STG_UNUSED,
                                            nat         new_cap STG_UNUSED)
 {
@@ -373,7 +373,7 @@ INLINE_HEADER void traceEventShutdown(Capability *cap STG_UNUSED)
     dtraceShutdown((EventCapNo)cap->no);
 }
 
-INLINE_HEADER void traceEventThreadWakeup(Capability *cap       STG_UNUSED, 
+INLINE_HEADER void traceEventThreadWakeup(Capability *cap       STG_UNUSED,
                                           StgTSO     *tso       STG_UNUSED,
                                           nat         other_cap STG_UNUSED)
 {
@@ -406,14 +406,14 @@ INLINE_HEADER void traceEventRequestParGc(Capability *cap STG_UNUSED)
     dtraceRequestParGc((EventCapNo)cap->no);
 }
 
-INLINE_HEADER void traceEventRunSpark(Capability *cap STG_UNUSED, 
+INLINE_HEADER void traceEventRunSpark(Capability *cap STG_UNUSED,
                                       StgTSO     *tso STG_UNUSED)
 {
     traceSparkEvent(cap, EVENT_RUN_SPARK, tso, 0);
     dtraceRunSpark((EventCapNo)cap->no, (EventThreadID)tso->id);
 }
 
-INLINE_HEADER void traceEventStealSpark(Capability *cap        STG_UNUSED, 
+INLINE_HEADER void traceEventStealSpark(Capability *cap        STG_UNUSED,
                                         StgTSO     *tso        STG_UNUSED,
                                         nat         victim_cap STG_UNUSED)
 {
@@ -422,7 +422,7 @@ INLINE_HEADER void traceEventStealSpark(Capability *cap        STG_UNUSED,
                      (EventCapNo)victim_cap);
 }
 
-INLINE_HEADER void traceEventCreateSparkThread(Capability  *cap      STG_UNUSED, 
+INLINE_HEADER void traceEventCreateSparkThread(Capability  *cap      STG_UNUSED,
                                                StgThreadID spark_tid STG_UNUSED)
 {
     traceSparkEvent(cap, EVENT_CREATE_SPARK_THREAD, 0, spark_tid);
diff --git a/rts/eventlog/EventLog.c b/rts/eventlog/EventLog.c
index 1e63a94..8ffefd2 100644
--- a/rts/eventlog/EventLog.c
+++ b/rts/eventlog/EventLog.c
@@ -88,7 +88,7 @@ char *EventDesc[] = {
   [EVENT_SPARK_COUNTERS]      = "Spark counters"
 };
 
-// Event type. 
+// Event type.
 
 typedef struct _EventType {
   EventTypeNum etNum;  // Event Type number.
@@ -114,7 +114,7 @@ static StgBool hasRoomForVariableEvent(EventsBuf *eb, nat payload_bytes);
 
 static inline void postWord8(EventsBuf *eb, StgWord8 i)
 {
-    *(eb->pos++) = i; 
+    *(eb->pos++) = i;
 }
 
 static inline void postWord16(EventsBuf *eb, StgWord16 i)
@@ -169,7 +169,7 @@ static inline void postEventHeader(EventsBuf *eb, EventTypeNum type)
 {
     postEventTypeNum(eb, type);
     postTimestamp(eb);
-}    
+}
 
 static inline void postInt8(EventsBuf *eb, StgInt8 i)
 { postWord8(eb, (StgWord8)i); }
@@ -213,17 +213,17 @@ initEventLogging(void)
     /* Open event log file for writing. */
     if ((event_log_file = fopen(event_log_filename, "wb")) == NULL) {
         sysErrorBelch("initEventLogging: can't open %s", event_log_filename);
-        stg_exit(EXIT_FAILURE);    
+        stg_exit(EXIT_FAILURE);
     }
 
-    /* 
+    /*
      * Allocate buffer(s) to store events.
      * Create buffer large enough for the header begin marker, all event
      * types, and header end marker to prevent checking if buffer has room
      * for each of these steps, and remove the need to flush the buffer to
      * disk during initialization.
      *
-     * Use a single buffer to store the header with event types, then flush 
+     * Use a single buffer to store the header with event types, then flush
      * the buffer so all buffers are empty for writing events.
      */
 #ifdef THREADED_RTS
@@ -320,7 +320,7 @@ initEventLogging(void)
             break;
 
         case EVENT_BLOCK_MARKER:
-            eventTypes[t].size = sizeof(StgWord32) + sizeof(EventTimestamp) + 
+            eventTypes[t].size = sizeof(StgWord32) + sizeof(EventTimestamp) +
                 sizeof(EventCapNo);
             break;
 
@@ -334,10 +334,10 @@ initEventLogging(void)
 
     // Mark end of event types in the header.
     postInt32(&eventBuf, EVENT_HET_END);
-    
+
     // Write in buffer: the header end marker.
     postInt32(&eventBuf, EVENT_HEADER_END);
-    
+
     // Prepare event buffer for events (data).
     postInt32(&eventBuf, EVENT_DATA_BEGIN);
 
@@ -380,14 +380,14 @@ endEventLogging(void)
     }
 }
 
-void 
+void
 freeEventLogging(void)
 {
     StgWord8 c;
-    
+
     // Free events buffer.
     for (c = 0; c < n_capabilities; ++c) {
-        if (capEventBuf[c].begin != NULL) 
+        if (capEventBuf[c].begin != NULL)
             stgFree(capEventBuf[c].begin);
     }
     if (capEventBuf != NULL)  {
@@ -398,7 +398,7 @@ freeEventLogging(void)
     }
 }
 
-void 
+void
 flushEventLog(void)
 {
     if (event_log_file != NULL) {
@@ -406,7 +406,7 @@ flushEventLog(void)
     }
 }
 
-void 
+void
 abortEventLogging(void)
 {
     freeEventLogging();
@@ -419,9 +419,9 @@ abortEventLogging(void)
  * If the buffer is full, prints out the buffer and clears it.
  */
 void
-postSchedEvent (Capability *cap, 
-                EventTypeNum tag, 
-                StgThreadID thread, 
+postSchedEvent (Capability *cap,
+                EventTypeNum tag,
+                StgThreadID thread,
                 StgWord info1,
                 StgWord info2)
 {
@@ -433,7 +433,7 @@ postSchedEvent (Capability *cap,
         // Flush event buffer to make room for new event.
         printAndClearEventBuf(eb);
     }
-    
+
     postEventHeader(eb, tag);
 
     switch (tag) {
@@ -484,7 +484,7 @@ postSchedEvent (Capability *cap,
 }
 
 void
-postSparkCountersEvent (Capability *cap, 
+postSparkCountersEvent (Capability *cap,
                         SparkCounters counters,
                         StgWord remaining)
 {
@@ -496,7 +496,7 @@ postSparkCountersEvent (Capability *cap,
         // Flush event buffer to make room for new event.
         printAndClearEventBuf(eb);
     }
-    
+
     postEventHeader(eb, EVENT_SPARK_COUNTERS);
     postWord64(eb,counters.created);
     postWord64(eb,counters.dud);
@@ -669,7 +669,7 @@ void postCapMsg(Capability *cap, char *msg, va_list ap)
 void postUserMsg(Capability *cap, char *msg, va_list ap)
 {
     postLogMsg(&capEventBuf[cap->no], EVENT_USER_MSG, msg, ap);
-}    
+}
 
 void postEventStartup(EventCapNo n_caps)
 {
@@ -730,7 +730,7 @@ void printAndClearEventBuf (EventsBuf *ebuf)
     if (ebuf->begin != NULL && ebuf->pos != ebuf->begin)
     {
         numBytes = ebuf->pos - ebuf->begin;
-        
+
         written = fwrite(ebuf->begin, 1, numBytes, event_log_file);
         if (written != numBytes) {
             debugBelch(
@@ -738,7 +738,7 @@ void printAndClearEventBuf (EventsBuf *ebuf)
                 " doesn't match numBytes=%" FMT_Word64, written, numBytes);
             return;
         }
-        
+
         resetEventsBuf(ebuf);
         flushCount++;
 
@@ -785,7 +785,7 @@ StgBool hasRoomForVariableEvent(EventsBuf *eb, nat payload_bytes)
   } else  {
       return 1; // Buf has enough space for the event.
   }
-}    
+}
 
 void postEventType(EventsBuf *eb, EventType *et)
 {
diff --git a/rts/eventlog/EventLog.h b/rts/eventlog/EventLog.h
index 4312927..b472a23 100644
--- a/rts/eventlog/EventLog.h
+++ b/rts/eventlog/EventLog.h
@@ -27,11 +27,11 @@ void freeEventLogging(void);
 void abortEventLogging(void); // #4512 - after fork child needs to abort
 void flushEventLog(void);     // event log inherited from parent
 
-/* 
+/*
  * Post a scheduler event to the capability's event buffer (an event
  * that has an associated thread).
  */
-void postSchedEvent(Capability *cap, EventTypeNum tag, 
+void postSchedEvent(Capability *cap, EventTypeNum tag,
                     StgThreadID id, StgWord info1, StgWord info2);
 
 /*
@@ -72,7 +72,7 @@ void postCapsetVecEvent (EventTypeNum tag,
 /*
  * Post an event with several counters relating to `par` sparks.
  */
-void postSparkCountersEvent (Capability *cap, 
+void postSparkCountersEvent (Capability *cap,
                              SparkCounters counters,
                              StgWord remaining);
 
@@ -89,12 +89,12 @@ INLINE_HEADER void postEvent (Capability *cap  STG_UNUSED,
                               EventTypeNum tag STG_UNUSED)
 { /* nothing */ }
 
-INLINE_HEADER void postMsg (char *msg STG_UNUSED, 
+INLINE_HEADER void postMsg (char *msg STG_UNUSED,
                             va_list ap STG_UNUSED)
 { /* nothing */ }
 
 INLINE_HEADER void postCapMsg (Capability *cap STG_UNUSED,
-                               char *msg STG_UNUSED, 
+                               char *msg STG_UNUSED,
                                va_list ap STG_UNUSED)
 { /* nothing */ }
 
-- 
1.7.4.1

